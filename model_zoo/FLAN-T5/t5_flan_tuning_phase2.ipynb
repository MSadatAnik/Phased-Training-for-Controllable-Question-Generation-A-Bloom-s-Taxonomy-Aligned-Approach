{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6d9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:44:11 | INFO | Starting PHASE 2: T5 Question Generation Training\n",
      "23:44:11 | INFO | Loading Phase 1 model from: ./t5-flan-question-generation-tuned/final...\n",
      "23:44:11 | INFO | Loading Phase 2 (Merged) datasets...\n",
      "23:44:11 | INFO | Loading Phase 2 Train data from: E:/A_CSE499/data\\merged_train.csv\n",
      "Checking Phase 2 Train:   0%|          | 0/90000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Checking Phase 2 Train: 100%|██████████| 90000/90000 [01:38<00:00, 916.94it/s]\n",
      "23:45:50 | INFO | --- DATA STATS FOR PHASE 2 TRAIN ---\n",
      "23:45:50 | INFO |   Total Rows:      90000\n",
      "23:45:50 | INFO |   Kept:            67260 (74.7%)\n",
      "23:45:50 | INFO |   Dropped (Empty): 0\n",
      "23:45:50 | INFO |   Dropped (>512):  22740 (25.3%)\n",
      "23:45:50 | INFO | ---------------------------------------\n",
      "23:45:50 | INFO | Loading Phase 2 Validation data from: E:/A_CSE499/data\\merged_val.csv\n",
      "Checking Phase 2 Validation: 100%|██████████| 10000/10000 [00:10<00:00, 910.73it/s]\n",
      "23:46:01 | INFO | --- DATA STATS FOR PHASE 2 VALIDATION ---\n",
      "23:46:01 | INFO |   Total Rows:      10000\n",
      "23:46:01 | INFO |   Kept:            7392 (73.9%)\n",
      "23:46:01 | INFO |   Dropped (Empty): 0\n",
      "23:46:01 | INFO |   Dropped (>512):  2608 (26.1%)\n",
      "23:46:01 | INFO | ---------------------------------------\n",
      "23:46:02 | INFO | Using default tokenizer.\n",
      "23:46:02 | INFO | === STARTING PHASE 2 HYPERPARAMETER TUNING ===\n",
      "[I 2025-11-20 23:46:02,350] Using an existing study with name 't5-phase2-tuning' instead of creating a new one.\n",
      "23:46:02 | INFO | Resuming study. 0 trials complete, running 10 more.\n",
      "23:46:02 | INFO | --- Starting Optuna Trial 1 ---\n",
      "23:46:02 | INFO | Trial 1: Using 67260 train samples and 2217 eval samples (subset for speed)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "W1120 23:47:30.689000 4184 Lib\\site-packages\\torch\\_inductor\\utils.py:1250] [17/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33632' max='33632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33632/33632 9:14:23, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:26:58 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval:   0%|          | 0/1109 [00:00<?, ?batch/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Group Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. To prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [31:20<00:00,  1.70s/batch]\n",
      "01:37:20 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:16<00:00,  1.64s/batch]\n",
      "02:46:32 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:25<00:00,  1.65s/batch]\n",
      "03:55:58 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:27<00:00,  1.54s/batch]\n",
      "05:03:22 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:28<00:00,  1.65s/batch]\n",
      "06:12:49 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:40<00:00,  1.66s/batch]\n",
      "07:22:26 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:17<00:00,  1.64s/batch]\n",
      "08:31:39 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:06<00:00,  1.63s/batch]\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "09:02:10 | INFO | --- Finished Optuna Trial 1 | Best BLEU-4: 0.09341728902236522 ---\n",
      "[I 2025-11-21 09:02:11,824] Trial 1 finished with value: 0.09341728902236522 and parameters: {'learning_rate': 2.368863950364079e-05, 'weight_decay': 0.09507143064099162, 'lr_scheduler_type': 'linear', 'warmup_steps': 740}. Best is trial 1 with value: 0.09341728902236522.\n",
      "09:02:11 | INFO | --- Starting Optuna Trial 2 ---\n",
      "09:02:12 | INFO | Trial 2: Using 67260 train samples and 2217 eval samples (subset for speed)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33632' max='33632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33632/33632 9:19:02, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:42:09 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:41<00:00,  1.66s/batch] \n",
      "10:51:54 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [31:53<00:00,  1.73s/batch]\n",
      "12:02:41 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [31:08<00:00,  1.69s/batch]\n",
      "13:12:44 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:57<00:00,  1.62s/batch]\n",
      "14:21:37 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:51<00:00,  1.67s/batch]\n",
      "15:31:24 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [31:05<00:00,  1.68s/batch]\n",
      "16:41:26 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:33<00:00,  1.65s/batch]\n",
      "17:50:58 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [31:06<00:00,  1.68s/batch]\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "18:22:34 | INFO | --- Finished Optuna Trial 2 | Best BLEU-4: 0.07762080887665193 ---\n",
      "[I 2025-11-21 18:22:35,500] Trial 2 finished with value: 0.07762080887665193 and parameters: {'learning_rate': 1.1430983876313214e-05, 'weight_decay': 0.08661761457749352, 'lr_scheduler_type': 'cosine', 'warmup_steps': 4080}. Best is trial 1 with value: 0.09341728902236522.\n",
      "18:22:35 | INFO | --- Starting Optuna Trial 3 ---\n",
      "18:22:36 | INFO | Trial 3: Using 67260 train samples and 2217 eval samples (subset for speed)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33632' max='33632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33632/33632 8:57:17, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:02:25 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:47<00:00,  1.67s/batch] \n",
      "20:12:11 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:31<00:00,  1.54s/batch]\n",
      "21:19:40 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:14<00:00,  1.53s/batch]\n",
      "22:26:55 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [25:54<00:00,  1.40s/batch]\n",
      "23:31:46 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:56<00:00,  1.62s/batch]\n",
      "00:40:42 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:19<00:00,  1.53s/batch]\n",
      "01:47:57 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:39<00:00,  1.55s/batch]\n",
      "02:55:33 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [25:08<00:00,  1.36s/batch]\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "03:21:12 | INFO | --- Finished Optuna Trial 3 | Best BLEU-4: 0.0893720270476521 ---\n",
      "[I 2025-11-22 03:21:12,911] Trial 3 finished with value: 0.0893720270476521 and parameters: {'learning_rate': 6.798962421591133e-05, 'weight_decay': 0.021233911067827616, 'lr_scheduler_type': 'constant', 'warmup_steps': 2253}. Best is trial 1 with value: 0.09341728902236522.\n",
      "03:21:13 | INFO | --- Starting Optuna Trial 4 ---\n",
      "03:21:13 | INFO | Trial 4: Using 67260 train samples and 2217 eval samples (subset for speed)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33632' max='33632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33632/33632 9:11:14, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:01:03 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [31:54<00:00,  1.73s/batch] \n",
      "05:11:51 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:14<00:00,  1.64s/batch]\n",
      "06:21:00 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:29<00:00,  1.65s/batch]\n",
      "07:30:25 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:15<00:00,  1.53s/batch]\n",
      "08:37:35 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:25<00:00,  1.65s/batch] \n",
      "09:46:53 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:56<00:00,  1.62s/batch]\n",
      "10:55:43 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:44<00:00,  1.61s/batch]\n",
      "12:04:22 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:59<00:00,  1.57s/batch]\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "12:33:46 | INFO | --- Finished Optuna Trial 4 | Best BLEU-4: 0.08463699197246195 ---\n",
      "[I 2025-11-22 12:33:47,482] Trial 4 finished with value: 0.08463699197246195 and parameters: {'learning_rate': 2.703616066662e-05, 'weight_decay': 0.029122914019804193, 'lr_scheduler_type': 'linear', 'warmup_steps': 1603}. Best is trial 1 with value: 0.09341728902236522.\n",
      "12:33:47 | INFO | --- Starting Optuna Trial 5 ---\n",
      "12:33:48 | INFO | Trial 5: Using 67260 train samples and 2217 eval samples (subset for speed)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33632' max='33632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33632/33632 9:07:24, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:13:38 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:58<00:00,  1.68s/batch] \n",
      "14:23:30 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:34<00:00,  1.60s/batch]\n",
      "15:31:56 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:44<00:00,  1.61s/batch]\n",
      "16:40:41 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [27:33<00:00,  1.49s/batch]\n",
      "17:47:09 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:58<00:00,  1.68s/batch] \n",
      "18:57:03 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:05<00:00,  1.57s/batch]\n",
      "20:05:03 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:09<00:00,  1.63s/batch]\n",
      "21:14:05 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [27:58<00:00,  1.51s/batch]\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "21:42:32 | INFO | --- Finished Optuna Trial 5 | Best BLEU-4: 0.08942902573821647 ---\n",
      "[I 2025-11-22 21:42:32,673] Trial 5 finished with value: 0.08942902573821647 and parameters: {'learning_rate': 2.858051065806938e-05, 'weight_decay': 0.07851759613930137, 'lr_scheduler_type': 'constant', 'warmup_steps': 290}. Best is trial 1 with value: 0.09341728902236522.\n",
      "21:42:32 | INFO | --- Starting Optuna Trial 6 ---\n",
      "21:42:33 | INFO | Trial 6: Using 67260 train samples and 2217 eval samples (subset for speed)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33632' max='33632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33632/33632 9:02:44, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:22:22 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:47<00:00,  1.67s/batch] \n",
      "23:32:06 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:40<00:00,  1.55s/batch]\n",
      "00:39:38 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:49<00:00,  1.56s/batch]\n",
      "01:47:26 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [26:46<00:00,  1.45s/batch]\n",
      "02:53:04 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:45<00:00,  1.66s/batch] \n",
      "04:02:43 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:49<00:00,  1.56s/batch]\n",
      "05:10:26 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:30<00:00,  1.60s/batch]\n",
      "06:18:54 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [27:17<00:00,  1.48s/batch]\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "06:46:36 | INFO | --- Finished Optuna Trial 6 | Best BLEU-4: 0.08932658124248248 ---\n",
      "[I 2025-11-23 06:46:37,194] Trial 6 finished with value: 0.08932658124248248 and parameters: {'learning_rate': 4.0508377813296754e-05, 'weight_decay': 0.017052412368729154, 'lr_scheduler_type': 'constant', 'warmup_steps': 3417}. Best is trial 1 with value: 0.09341728902236522.\n",
      "06:46:37 | INFO | --- Starting Optuna Trial 7 ---\n",
      "06:46:37 | INFO | Trial 7: Using 67260 train samples and 2217 eval samples (subset for speed)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33632' max='33632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33632/33632 9:14:37, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:26:27 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [31:25<00:00,  1.70s/batch] \n",
      "08:36:46 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:42<00:00,  1.66s/batch]\n",
      "09:46:22 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:13<00:00,  1.63s/batch]\n",
      "10:55:27 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:15<00:00,  1.58s/batch]\n",
      "12:03:41 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:39<00:00,  1.66s/batch]\n",
      "13:13:12 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:33<00:00,  1.65s/batch]\n",
      "14:22:42 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:22<00:00,  1.64s/batch]\n",
      "15:31:59 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:09<00:00,  1.63s/batch]\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "16:02:34 | INFO | --- Finished Optuna Trial 7 | Best BLEU-4: 0.08197044661495713 ---\n",
      "[I 2025-11-23 16:02:34,706] Trial 7 finished with value: 0.08197044661495713 and parameters: {'learning_rate': 2.0165721691808572e-05, 'weight_decay': 0.009767211400638388, 'lr_scheduler_type': 'linear', 'warmup_steps': 2132}. Best is trial 1 with value: 0.09341728902236522.\n",
      "16:02:34 | INFO | --- Starting Optuna Trial 8 ---\n",
      "16:02:35 | INFO | Trial 8: Using 67260 train samples and 2217 eval samples (subset for speed)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33632' max='33632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33632/33632 9:20:18, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:42:24 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:50<00:00,  1.67s/batch] \n",
      "17:52:12 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [32:18<00:00,  1.75s/batch]\n",
      "19:03:28 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:48<00:00,  1.67s/batch]\n",
      "20:13:16 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:12<00:00,  1.63s/batch]\n",
      "21:22:22 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [31:01<00:00,  1.68s/batch] \n",
      "22:32:19 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [31:24<00:00,  1.70s/batch]\n",
      "23:42:43 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:44<00:00,  1.66s/batch]\n",
      "00:52:26 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [31:14<00:00,  1.69s/batch]\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "01:24:12 | INFO | --- Finished Optuna Trial 8 | Best BLEU-4: 0.07781895063444473 ---\n",
      "[I 2025-11-24 01:24:12,831] Trial 8 finished with value: 0.07781895063444473 and parameters: {'learning_rate': 1.0824018381500954e-05, 'weight_decay': 0.0909320402078782, 'lr_scheduler_type': 'cosine', 'warmup_steps': 2234}. Best is trial 1 with value: 0.09341728902236522.\n",
      "01:24:12 | INFO | --- Starting Optuna Trial 9 ---\n",
      "01:24:13 | INFO | Trial 9: Using 67260 train samples and 2217 eval samples (subset for speed)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33632' max='33632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33632/33632 9:08:09, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:04:03 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [32:03<00:00,  1.73s/batch] \n",
      "03:15:02 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:15<00:00,  1.58s/batch]\n",
      "04:23:12 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:59<00:00,  1.62s/batch]\n",
      "05:32:03 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [27:50<00:00,  1.51s/batch]\n",
      "06:38:46 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [30:03<00:00,  1.63s/batch]\n",
      "07:47:41 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:09<00:00,  1.58s/batch]\n",
      "08:55:45 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:34<00:00,  1.60s/batch]\n",
      "10:04:17 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:55<00:00,  1.56s/batch]\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "10:33:41 | INFO | --- Finished Optuna Trial 9 | Best BLEU-4: 0.08708127105978686 ---\n",
      "[I 2025-11-24 10:33:42,427] Trial 9 finished with value: 0.08708127105978686 and parameters: {'learning_rate': 3.521358805467871e-05, 'weight_decay': 0.018485445552552705, 'lr_scheduler_type': 'linear', 'warmup_steps': 3772}. Best is trial 1 with value: 0.09341728902236522.\n",
      "10:33:42 | INFO | --- Starting Optuna Trial 10 ---\n",
      "10:33:43 | INFO | Trial 10: Using 67260 train samples and 2217 eval samples (subset for speed)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33632' max='33632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33632/33632 9:05:25, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:13:28 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [31:29<00:00,  1.70s/batch]\n",
      "12:23:55 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:55<00:00,  1.57s/batch]\n",
      "13:31:48 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:18<00:00,  1.59s/batch]\n",
      "14:40:02 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [27:22<00:00,  1.48s/batch]\n",
      "15:46:20 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:46<00:00,  1.61s/batch]\n",
      "16:55:06 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [29:23<00:00,  1.59s/batch]\n",
      "18:03:28 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:56<00:00,  1.57s/batch]\n",
      "19:11:21 | INFO | === TUNING TRIAL EVALUATION (BLEU-4 Only) ===\n",
      "Tuning Trial Eval: 100%|██████████| 1109/1109 [28:41<00:00,  1.55s/batch]\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "19:40:27 | INFO | --- Finished Optuna Trial 10 | Best BLEU-4: 0.08660428110677164 ---\n",
      "[I 2025-11-24 19:40:28,344] Trial 10 finished with value: 0.08660428110677164 and parameters: {'learning_rate': 3.961867790406586e-05, 'weight_decay': 0.09218742350231168, 'lr_scheduler_type': 'cosine', 'warmup_steps': 1435}. Best is trial 1 with value: 0.09341728902236522.\n",
      "19:40:28 | INFO | === TUNING COMPLETE ===\n",
      "19:40:28 | INFO | Best parameters: {\n",
      "  \"learning_rate\": 2.368863950364079e-05,\n",
      "  \"weight_decay\": 0.09507143064099162,\n",
      "  \"lr_scheduler_type\": \"linear\",\n",
      "  \"warmup_steps\": 740\n",
      "}\n",
      "19:40:28 | INFO | === STARTING PHASE 2 FINAL TRAINING ===\n",
      "19:40:29 | INFO | Training Configuration: Dataset size: 67,260, Effective batch size: 16, Steps per epoch: 4203\n",
      "19:40:37 | INFO | Starting Phase 2 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33632' max='33632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33632/33632 23:38:20, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:45:41 | INFO | Epoch 0.10 | Loss: 1.7660 | LR: 1.34e-05\n",
      "19:49:31 | INFO | Epoch 0.20 | Loss: 1.5576 | LR: 2.36e-05\n",
      "19:53:23 | INFO | Epoch 0.30 | Loss: 1.4929 | LR: 2.33e-05\n",
      "19:57:15 | INFO | Epoch 0.40 | Loss: 1.4635 | LR: 2.30e-05\n",
      "20:01:05 | INFO | Epoch 0.50 | Loss: 1.4497 | LR: 2.27e-05\n",
      "20:05:00 | INFO | Epoch 0.60 | Loss: 1.4319 | LR: 2.24e-05\n",
      "20:08:50 | INFO | Epoch 0.70 | Loss: 1.4148 | LR: 2.21e-05\n",
      "20:12:43 | INFO | Epoch 0.80 | Loss: 1.4173 | LR: 2.18e-05\n",
      "20:16:34 | INFO | Epoch 0.90 | Loss: 1.3920 | LR: 2.15e-05\n",
      "20:20:22 | INFO | Epoch 1.00 | Loss: 1.3890 | LR: 2.12e-05\n",
      "20:20:24 | INFO | ===== Average Training Loss for Epoch 1: 1.4775 =====\n",
      "20:20:24 | INFO | === BEFORE EVALUATION ===\n",
      "20:20:24 | INFO | BEFORE EVAL GPU Memory - Allocated: 2.80GB, Reserved: 6.36GB\n",
      "20:20:24 | INFO | BEFORE EVAL Model - Training mode: True, Device: cuda:0\n",
      "20:20:32 | INFO | Starting evaluation on full validation set...\n",
      "Evaluating Epoch 1: 100%|██████████| 3696/3696 [1:45:25<00:00,  1.71s/batch]  \n",
      "22:05:58 | INFO | Computing metrics for 7392 predictions...\n",
      "22:05:58 | INFO | Computing quality and diversity metrics...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - Epoch 1\n",
      "================================================================================\n",
      "\n",
      "QUALITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| BLEU-1     |  0.2363 |\n",
      "+------------+---------+\n",
      "| BLEU-2     |  0.1375 |\n",
      "+------------+---------+\n",
      "| BLEU-3     |  0.0946 |\n",
      "+------------+---------+\n",
      "| BLEU-4     |  0.0714 |\n",
      "+------------+---------+\n",
      "| ROUGE-L    |  0.2576 |\n",
      "+------------+---------+\n",
      "| METEOR     |  0.2824 |\n",
      "+------------+---------+\n",
      "| BERT-SCORE |  0.8836 |\n",
      "+------------+---------+\n",
      "| SELF-BLEU  |  0.9166 |\n",
      "+------------+---------+\n",
      "\n",
      "DIVERSITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| SELF-BLEU  |  0.9166 |\n",
      "+------------+---------+\n",
      "| DISTINCT-1 |  0.122  |\n",
      "+------------+---------+\n",
      "| DISTINCT-2 |  0.3106 |\n",
      "+------------+---------+\n",
      "\n",
      "OTHER METRICS:\n",
      "+----------+---------+\n",
      "| Metric   |   Score |\n",
      "+==========+=========+\n",
      "| LOSS     |  1.3436 |\n",
      "+----------+---------+\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:42:42 | INFO | === AFTER EVALUATION ===\n",
      "22:42:42 | INFO | AFTER EVAL GPU Memory - Allocated: 2.81GB, Reserved: 3.08GB\n",
      "22:42:42 | INFO | AFTER EVAL Model - Training mode: True, Device: cuda:0\n",
      "22:42:42 | INFO | Evaluation completed successfully. Average loss: 1.3436\n",
      "22:46:54 | INFO | Epoch 1.10 | Loss: 1.3584 | LR: 2.09e-05\n",
      "22:50:45 | INFO | Epoch 1.20 | Loss: 1.3341 | LR: 2.06e-05\n",
      "22:54:35 | INFO | Epoch 1.30 | Loss: 1.3365 | LR: 2.03e-05\n",
      "22:58:28 | INFO | Epoch 1.40 | Loss: 1.3107 | LR: 2.00e-05\n",
      "23:02:17 | INFO | Epoch 1.50 | Loss: 1.3283 | LR: 1.97e-05\n",
      "23:06:11 | INFO | Epoch 1.60 | Loss: 1.3167 | LR: 1.94e-05\n",
      "23:10:02 | INFO | Epoch 1.70 | Loss: 1.3297 | LR: 1.91e-05\n",
      "23:13:53 | INFO | Epoch 1.80 | Loss: 1.3042 | LR: 1.88e-05\n",
      "23:17:45 | INFO | Epoch 1.90 | Loss: 1.2927 | LR: 1.85e-05\n",
      "23:21:34 | INFO | Epoch 2.00 | Loss: 1.2909 | LR: 1.82e-05\n",
      "23:21:38 | INFO | ===== Average Training Loss for Epoch 2: 1.3202 =====\n",
      "23:21:38 | INFO | === BEFORE EVALUATION ===\n",
      "23:21:38 | INFO | BEFORE EVAL GPU Memory - Allocated: 2.80GB, Reserved: 6.47GB\n",
      "23:21:38 | INFO | BEFORE EVAL Model - Training mode: True, Device: cuda:0\n",
      "23:21:46 | INFO | Starting evaluation on full validation set...\n",
      "Evaluating Epoch 2: 100%|██████████| 3696/3696 [1:41:53<00:00,  1.65s/batch]  \n",
      "01:03:39 | INFO | Computing metrics for 7392 predictions...\n",
      "01:03:39 | INFO | Computing quality and diversity metrics...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - Epoch 2\n",
      "================================================================================\n",
      "\n",
      "QUALITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| BLEU-1     |  0.245  |\n",
      "+------------+---------+\n",
      "| BLEU-2     |  0.146  |\n",
      "+------------+---------+\n",
      "| BLEU-3     |  0.1033 |\n",
      "+------------+---------+\n",
      "| BLEU-4     |  0.0795 |\n",
      "+------------+---------+\n",
      "| ROUGE-L    |  0.2651 |\n",
      "+------------+---------+\n",
      "| METEOR     |  0.2931 |\n",
      "+------------+---------+\n",
      "| BERT-SCORE |  0.8847 |\n",
      "+------------+---------+\n",
      "| SELF-BLEU  |  0.9165 |\n",
      "+------------+---------+\n",
      "\n",
      "DIVERSITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| SELF-BLEU  |  0.9165 |\n",
      "+------------+---------+\n",
      "| DISTINCT-1 |  0.1213 |\n",
      "+------------+---------+\n",
      "| DISTINCT-2 |  0.3043 |\n",
      "+------------+---------+\n",
      "\n",
      "OTHER METRICS:\n",
      "+----------+---------+\n",
      "| Metric   |   Score |\n",
      "+==========+=========+\n",
      "| LOSS     |  1.3139 |\n",
      "+----------+---------+\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:39:29 | INFO | === AFTER EVALUATION ===\n",
      "01:39:29 | INFO | AFTER EVAL GPU Memory - Allocated: 2.82GB, Reserved: 3.08GB\n",
      "01:39:29 | INFO | AFTER EVAL Model - Training mode: True, Device: cuda:0\n",
      "01:39:29 | INFO | Evaluation completed successfully. Average loss: 1.3139\n",
      "01:43:39 | INFO | Epoch 2.10 | Loss: 1.2534 | LR: 1.79e-05\n",
      "01:47:30 | INFO | Epoch 2.20 | Loss: 1.2744 | LR: 1.76e-05\n",
      "01:51:20 | INFO | Epoch 2.30 | Loss: 1.2677 | LR: 1.73e-05\n",
      "01:55:15 | INFO | Epoch 2.40 | Loss: 1.2544 | LR: 1.70e-05\n",
      "01:59:04 | INFO | Epoch 2.50 | Loss: 1.2575 | LR: 1.67e-05\n",
      "02:02:57 | INFO | Epoch 2.60 | Loss: 1.2487 | LR: 1.64e-05\n",
      "02:06:49 | INFO | Epoch 2.70 | Loss: 1.2430 | LR: 1.61e-05\n",
      "02:10:38 | INFO | Epoch 2.80 | Loss: 1.2484 | LR: 1.58e-05\n",
      "02:14:33 | INFO | Epoch 2.90 | Loss: 1.2347 | LR: 1.55e-05\n",
      "02:18:24 | INFO | Epoch 3.00 | Loss: 1.2395 | LR: 1.51e-05\n",
      "02:18:29 | INFO | ===== Average Training Loss for Epoch 3: 1.2522 =====\n",
      "02:18:29 | INFO | === BEFORE EVALUATION ===\n",
      "02:18:29 | INFO | BEFORE EVAL GPU Memory - Allocated: 2.80GB, Reserved: 6.37GB\n",
      "02:18:29 | INFO | BEFORE EVAL Model - Training mode: True, Device: cuda:0\n",
      "02:18:37 | INFO | Starting evaluation on full validation set...\n",
      "Evaluating Epoch 3: 100%|██████████| 3696/3696 [1:43:38<00:00,  1.68s/batch]  \n",
      "04:02:15 | INFO | Computing metrics for 7392 predictions...\n",
      "04:02:15 | INFO | Computing quality and diversity metrics...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - Epoch 3\n",
      "================================================================================\n",
      "\n",
      "QUALITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| BLEU-1     |  0.2543 |\n",
      "+------------+---------+\n",
      "| BLEU-2     |  0.1545 |\n",
      "+------------+---------+\n",
      "| BLEU-3     |  0.109  |\n",
      "+------------+---------+\n",
      "| BLEU-4     |  0.0835 |\n",
      "+------------+---------+\n",
      "| ROUGE-L    |  0.274  |\n",
      "+------------+---------+\n",
      "| METEOR     |  0.3013 |\n",
      "+------------+---------+\n",
      "| BERT-SCORE |  0.8872 |\n",
      "+------------+---------+\n",
      "| SELF-BLEU  |  0.9173 |\n",
      "+------------+---------+\n",
      "\n",
      "DIVERSITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| SELF-BLEU  |  0.9173 |\n",
      "+------------+---------+\n",
      "| DISTINCT-1 |  0.1231 |\n",
      "+------------+---------+\n",
      "| DISTINCT-2 |  0.3091 |\n",
      "+------------+---------+\n",
      "\n",
      "OTHER METRICS:\n",
      "+----------+---------+\n",
      "| Metric   |   Score |\n",
      "+==========+=========+\n",
      "| LOSS     |  1.3077 |\n",
      "+----------+---------+\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:37:44 | INFO | === AFTER EVALUATION ===\n",
      "04:37:44 | INFO | AFTER EVAL GPU Memory - Allocated: 2.81GB, Reserved: 3.07GB\n",
      "04:37:44 | INFO | AFTER EVAL Model - Training mode: True, Device: cuda:0\n",
      "04:37:44 | INFO | Evaluation completed successfully. Average loss: 1.3077\n",
      "04:41:50 | INFO | Epoch 3.10 | Loss: 1.2085 | LR: 1.48e-05\n",
      "04:45:42 | INFO | Epoch 3.20 | Loss: 1.1894 | LR: 1.45e-05\n",
      "04:49:29 | INFO | Epoch 3.30 | Loss: 1.2188 | LR: 1.42e-05\n",
      "04:53:23 | INFO | Epoch 3.40 | Loss: 1.2114 | LR: 1.39e-05\n",
      "04:57:13 | INFO | Epoch 3.50 | Loss: 1.1973 | LR: 1.36e-05\n",
      "05:01:05 | INFO | Epoch 3.60 | Loss: 1.2033 | LR: 1.33e-05\n",
      "05:04:58 | INFO | Epoch 3.70 | Loss: 1.1939 | LR: 1.30e-05\n",
      "05:08:48 | INFO | Epoch 3.80 | Loss: 1.2085 | LR: 1.27e-05\n",
      "05:12:42 | INFO | Epoch 3.90 | Loss: 1.2060 | LR: 1.24e-05\n",
      "05:16:33 | INFO | Epoch 4.00 | Loss: 1.1912 | LR: 1.21e-05\n",
      "05:16:41 | INFO | ===== Average Training Loss for Epoch 4: 1.2028 =====\n",
      "05:16:41 | INFO | === BEFORE EVALUATION ===\n",
      "05:16:41 | INFO | BEFORE EVAL GPU Memory - Allocated: 2.80GB, Reserved: 6.27GB\n",
      "05:16:41 | INFO | BEFORE EVAL Model - Training mode: True, Device: cuda:0\n",
      "05:16:48 | INFO | Starting evaluation on full validation set...\n",
      "Evaluating Epoch 4: 100%|██████████| 3696/3696 [1:44:30<00:00,  1.70s/batch]  \n",
      "07:01:19 | INFO | Computing metrics for 7392 predictions...\n",
      "07:01:19 | INFO | Computing quality and diversity metrics...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - Epoch 4\n",
      "================================================================================\n",
      "\n",
      "QUALITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| BLEU-1     |  0.2518 |\n",
      "+------------+---------+\n",
      "| BLEU-2     |  0.1522 |\n",
      "+------------+---------+\n",
      "| BLEU-3     |  0.108  |\n",
      "+------------+---------+\n",
      "| BLEU-4     |  0.0834 |\n",
      "+------------+---------+\n",
      "| ROUGE-L    |  0.2685 |\n",
      "+------------+---------+\n",
      "| METEOR     |  0.3008 |\n",
      "+------------+---------+\n",
      "| BERT-SCORE |  0.8856 |\n",
      "+------------+---------+\n",
      "| SELF-BLEU  |  0.9173 |\n",
      "+------------+---------+\n",
      "\n",
      "DIVERSITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| SELF-BLEU  |  0.9173 |\n",
      "+------------+---------+\n",
      "| DISTINCT-1 |  0.1213 |\n",
      "+------------+---------+\n",
      "| DISTINCT-2 |  0.3069 |\n",
      "+------------+---------+\n",
      "\n",
      "OTHER METRICS:\n",
      "+----------+---------+\n",
      "| Metric   |   Score |\n",
      "+==========+=========+\n",
      "| LOSS     |  1.3057 |\n",
      "+----------+---------+\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:37:25 | INFO | === AFTER EVALUATION ===\n",
      "07:37:25 | INFO | AFTER EVAL GPU Memory - Allocated: 2.82GB, Reserved: 3.08GB\n",
      "07:37:25 | INFO | AFTER EVAL Model - Training mode: True, Device: cuda:0\n",
      "07:37:25 | INFO | Evaluation completed successfully. Average loss: 1.3057\n",
      "07:41:27 | INFO | Epoch 4.10 | Loss: 1.1860 | LR: 1.18e-05\n",
      "07:45:20 | INFO | Epoch 4.20 | Loss: 1.1604 | LR: 1.15e-05\n",
      "07:49:08 | INFO | Epoch 4.30 | Loss: 1.1794 | LR: 1.12e-05\n",
      "07:53:01 | INFO | Epoch 4.40 | Loss: 1.1618 | LR: 1.09e-05\n",
      "07:56:52 | INFO | Epoch 4.50 | Loss: 1.1543 | LR: 1.06e-05\n",
      "08:00:42 | INFO | Epoch 4.60 | Loss: 1.1781 | LR: 1.03e-05\n",
      "08:04:36 | INFO | Epoch 4.70 | Loss: 1.1435 | LR: 1.00e-05\n",
      "08:08:25 | INFO | Epoch 4.80 | Loss: 1.1682 | LR: 9.70e-06\n",
      "08:12:19 | INFO | Epoch 4.90 | Loss: 1.1638 | LR: 9.40e-06\n",
      "08:16:09 | INFO | Epoch 5.00 | Loss: 1.1679 | LR: 9.10e-06\n",
      "08:16:18 | INFO | ===== Average Training Loss for Epoch 5: 1.1663 =====\n",
      "08:16:18 | INFO | === BEFORE EVALUATION ===\n",
      "08:16:18 | INFO | BEFORE EVAL GPU Memory - Allocated: 2.80GB, Reserved: 6.37GB\n",
      "08:16:18 | INFO | BEFORE EVAL Model - Training mode: True, Device: cuda:0\n",
      "08:16:26 | INFO | Starting evaluation on full validation set...\n",
      "Evaluating Epoch 5: 100%|██████████| 3696/3696 [1:42:03<00:00,  1.66s/batch]  \n",
      "09:58:29 | INFO | Computing metrics for 7392 predictions...\n",
      "09:58:29 | INFO | Computing quality and diversity metrics...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - Epoch 5\n",
      "================================================================================\n",
      "\n",
      "QUALITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| BLEU-1     |  0.2569 |\n",
      "+------------+---------+\n",
      "| BLEU-2     |  0.1567 |\n",
      "+------------+---------+\n",
      "| BLEU-3     |  0.1114 |\n",
      "+------------+---------+\n",
      "| BLEU-4     |  0.0861 |\n",
      "+------------+---------+\n",
      "| ROUGE-L    |  0.2727 |\n",
      "+------------+---------+\n",
      "| METEOR     |  0.3048 |\n",
      "+------------+---------+\n",
      "| BERT-SCORE |  0.8867 |\n",
      "+------------+---------+\n",
      "| SELF-BLEU  |  0.9163 |\n",
      "+------------+---------+\n",
      "\n",
      "DIVERSITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| SELF-BLEU  |  0.9163 |\n",
      "+------------+---------+\n",
      "| DISTINCT-1 |  0.1189 |\n",
      "+------------+---------+\n",
      "| DISTINCT-2 |  0.3    |\n",
      "+------------+---------+\n",
      "\n",
      "OTHER METRICS:\n",
      "+----------+---------+\n",
      "| Metric   |   Score |\n",
      "+==========+=========+\n",
      "| LOSS     |  1.3046 |\n",
      "+----------+---------+\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:34:12 | INFO | === AFTER EVALUATION ===\n",
      "10:34:12 | INFO | AFTER EVAL GPU Memory - Allocated: 2.82GB, Reserved: 3.08GB\n",
      "10:34:12 | INFO | AFTER EVAL Model - Training mode: True, Device: cuda:0\n",
      "10:34:12 | INFO | Evaluation completed successfully. Average loss: 1.3046\n",
      "10:38:12 | INFO | Epoch 5.10 | Loss: 1.1329 | LR: 8.80e-06\n",
      "10:42:06 | INFO | Epoch 5.20 | Loss: 1.1313 | LR: 8.49e-06\n",
      "10:45:56 | INFO | Epoch 5.29 | Loss: 1.1428 | LR: 8.19e-06\n",
      "10:49:49 | INFO | Epoch 5.39 | Loss: 1.1464 | LR: 7.89e-06\n",
      "10:53:42 | INFO | Epoch 5.49 | Loss: 1.1411 | LR: 7.59e-06\n",
      "10:57:30 | INFO | Epoch 5.59 | Loss: 1.1425 | LR: 7.28e-06\n",
      "11:01:24 | INFO | Epoch 5.69 | Loss: 1.1428 | LR: 6.98e-06\n",
      "11:05:15 | INFO | Epoch 5.79 | Loss: 1.1361 | LR: 6.68e-06\n",
      "11:09:07 | INFO | Epoch 5.89 | Loss: 1.1428 | LR: 6.38e-06\n",
      "11:12:59 | INFO | Epoch 5.99 | Loss: 1.1516 | LR: 6.07e-06\n",
      "11:13:09 | INFO | ===== Average Training Loss for Epoch 6: 1.1410 =====\n",
      "11:13:09 | INFO | === BEFORE EVALUATION ===\n",
      "11:13:09 | INFO | BEFORE EVAL GPU Memory - Allocated: 2.80GB, Reserved: 6.28GB\n",
      "11:13:09 | INFO | BEFORE EVAL Model - Training mode: True, Device: cuda:0\n",
      "11:13:17 | INFO | Starting evaluation on full validation set...\n",
      "Evaluating Epoch 6: 100%|██████████| 3696/3696 [1:43:32<00:00,  1.68s/batch]  \n",
      "12:56:49 | INFO | Computing metrics for 7392 predictions...\n",
      "12:56:49 | INFO | Computing quality and diversity metrics...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - Epoch 6\n",
      "================================================================================\n",
      "\n",
      "QUALITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| BLEU-1     |  0.2579 |\n",
      "+------------+---------+\n",
      "| BLEU-2     |  0.1579 |\n",
      "+------------+---------+\n",
      "| BLEU-3     |  0.1125 |\n",
      "+------------+---------+\n",
      "| BLEU-4     |  0.0871 |\n",
      "+------------+---------+\n",
      "| ROUGE-L    |  0.2743 |\n",
      "+------------+---------+\n",
      "| METEOR     |  0.3082 |\n",
      "+------------+---------+\n",
      "| BERT-SCORE |  0.8873 |\n",
      "+------------+---------+\n",
      "| SELF-BLEU  |  0.9161 |\n",
      "+------------+---------+\n",
      "\n",
      "DIVERSITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| SELF-BLEU  |  0.9161 |\n",
      "+------------+---------+\n",
      "| DISTINCT-1 |  0.1199 |\n",
      "+------------+---------+\n",
      "| DISTINCT-2 |  0.3036 |\n",
      "+------------+---------+\n",
      "\n",
      "OTHER METRICS:\n",
      "+----------+---------+\n",
      "| Metric   |   Score |\n",
      "+==========+=========+\n",
      "| LOSS     |  1.3032 |\n",
      "+----------+---------+\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:33:03 | INFO | === AFTER EVALUATION ===\n",
      "13:33:03 | INFO | AFTER EVAL GPU Memory - Allocated: 2.82GB, Reserved: 3.08GB\n",
      "13:33:03 | INFO | AFTER EVAL Model - Training mode: True, Device: cuda:0\n",
      "13:33:03 | INFO | Evaluation completed successfully. Average loss: 1.3032\n",
      "13:37:04 | INFO | Epoch 6.09 | Loss: 1.1304 | LR: 5.77e-06\n",
      "13:40:57 | INFO | Epoch 6.19 | Loss: 1.1122 | LR: 5.47e-06\n",
      "13:44:48 | INFO | Epoch 6.29 | Loss: 1.1196 | LR: 5.17e-06\n",
      "13:48:39 | INFO | Epoch 6.39 | Loss: 1.1175 | LR: 4.86e-06\n",
      "13:52:32 | INFO | Epoch 6.49 | Loss: 1.1300 | LR: 4.56e-06\n",
      "13:56:22 | INFO | Epoch 6.59 | Loss: 1.1201 | LR: 4.26e-06\n",
      "14:00:15 | INFO | Epoch 6.69 | Loss: 1.1208 | LR: 3.96e-06\n",
      "14:04:07 | INFO | Epoch 6.79 | Loss: 1.1127 | LR: 3.65e-06\n",
      "14:07:58 | INFO | Epoch 6.89 | Loss: 1.1327 | LR: 3.35e-06\n",
      "14:11:51 | INFO | Epoch 6.99 | Loss: 1.1173 | LR: 3.05e-06\n",
      "14:12:03 | INFO | ===== Average Training Loss for Epoch 7: 1.1213 =====\n",
      "14:12:03 | INFO | === BEFORE EVALUATION ===\n",
      "14:12:03 | INFO | BEFORE EVAL GPU Memory - Allocated: 2.80GB, Reserved: 6.31GB\n",
      "14:12:03 | INFO | BEFORE EVAL Model - Training mode: True, Device: cuda:0\n",
      "14:12:11 | INFO | Starting evaluation on full validation set...\n",
      "Evaluating Epoch 7: 100%|██████████| 3696/3696 [1:38:42<00:00,  1.60s/batch]  \n",
      "15:50:54 | INFO | Computing metrics for 7392 predictions...\n",
      "15:50:54 | INFO | Computing quality and diversity metrics...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - Epoch 7\n",
      "================================================================================\n",
      "\n",
      "QUALITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| BLEU-1     |  0.2592 |\n",
      "+------------+---------+\n",
      "| BLEU-2     |  0.1586 |\n",
      "+------------+---------+\n",
      "| BLEU-3     |  0.1131 |\n",
      "+------------+---------+\n",
      "| BLEU-4     |  0.0875 |\n",
      "+------------+---------+\n",
      "| ROUGE-L    |  0.2762 |\n",
      "+------------+---------+\n",
      "| METEOR     |  0.3071 |\n",
      "+------------+---------+\n",
      "| BERT-SCORE |  0.8871 |\n",
      "+------------+---------+\n",
      "| SELF-BLEU  |  0.9162 |\n",
      "+------------+---------+\n",
      "\n",
      "DIVERSITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| SELF-BLEU  |  0.9162 |\n",
      "+------------+---------+\n",
      "| DISTINCT-1 |  0.1203 |\n",
      "+------------+---------+\n",
      "| DISTINCT-2 |  0.3025 |\n",
      "+------------+---------+\n",
      "\n",
      "OTHER METRICS:\n",
      "+----------+---------+\n",
      "| Metric   |   Score |\n",
      "+==========+=========+\n",
      "| LOSS     |  1.3025 |\n",
      "+----------+---------+\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:26:13 | INFO | === AFTER EVALUATION ===\n",
      "16:26:13 | INFO | AFTER EVAL GPU Memory - Allocated: 2.82GB, Reserved: 3.08GB\n",
      "16:26:13 | INFO | AFTER EVAL Model - Training mode: True, Device: cuda:0\n",
      "16:26:13 | INFO | Evaluation completed successfully. Average loss: 1.3025\n",
      "16:30:13 | INFO | Epoch 7.09 | Loss: 1.1222 | LR: 2.75e-06\n",
      "16:34:06 | INFO | Epoch 7.19 | Loss: 1.0950 | LR: 2.44e-06\n",
      "16:37:58 | INFO | Epoch 7.29 | Loss: 1.1060 | LR: 2.14e-06\n",
      "16:41:46 | INFO | Epoch 7.39 | Loss: 1.1134 | LR: 1.84e-06\n",
      "16:45:40 | INFO | Epoch 7.49 | Loss: 1.1026 | LR: 1.54e-06\n",
      "16:49:31 | INFO | Epoch 7.59 | Loss: 1.1228 | LR: 1.23e-06\n",
      "16:53:23 | INFO | Epoch 7.69 | Loss: 1.1080 | LR: 9.31e-07\n",
      "16:57:14 | INFO | Epoch 7.79 | Loss: 1.1116 | LR: 6.29e-07\n",
      "17:01:03 | INFO | Epoch 7.89 | Loss: 1.1096 | LR: 3.26e-07\n",
      "17:04:57 | INFO | Epoch 7.99 | Loss: 1.1057 | LR: 2.38e-08\n",
      "17:05:12 | INFO | ===== Average Training Loss for Epoch 8: 1.1097 =====\n",
      "17:05:12 | INFO | === BEFORE EVALUATION ===\n",
      "17:05:12 | INFO | BEFORE EVAL GPU Memory - Allocated: 2.80GB, Reserved: 6.40GB\n",
      "17:05:12 | INFO | BEFORE EVAL Model - Training mode: True, Device: cuda:0\n",
      "17:05:20 | INFO | Starting evaluation on full validation set...\n",
      "Evaluating Epoch 8: 100%|██████████| 3696/3696 [1:39:05<00:00,  1.61s/batch]  \n",
      "18:44:25 | INFO | Computing metrics for 7392 predictions...\n",
      "18:44:25 | INFO | Computing quality and diversity metrics...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - Epoch 8\n",
      "================================================================================\n",
      "\n",
      "QUALITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| BLEU-1     |  0.2583 |\n",
      "+------------+---------+\n",
      "| BLEU-2     |  0.1578 |\n",
      "+------------+---------+\n",
      "| BLEU-3     |  0.1123 |\n",
      "+------------+---------+\n",
      "| BLEU-4     |  0.087  |\n",
      "+------------+---------+\n",
      "| ROUGE-L    |  0.2746 |\n",
      "+------------+---------+\n",
      "| METEOR     |  0.3065 |\n",
      "+------------+---------+\n",
      "| BERT-SCORE |  0.887  |\n",
      "+------------+---------+\n",
      "| SELF-BLEU  |  0.917  |\n",
      "+------------+---------+\n",
      "\n",
      "DIVERSITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| SELF-BLEU  |  0.917  |\n",
      "+------------+---------+\n",
      "| DISTINCT-1 |  0.1204 |\n",
      "+------------+---------+\n",
      "| DISTINCT-2 |  0.3038 |\n",
      "+------------+---------+\n",
      "\n",
      "OTHER METRICS:\n",
      "+----------+---------+\n",
      "| Metric   |   Score |\n",
      "+==========+=========+\n",
      "| LOSS     |  1.3066 |\n",
      "+----------+---------+\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:19:46 | INFO | === AFTER EVALUATION ===\n",
      "19:19:46 | INFO | AFTER EVAL GPU Memory - Allocated: 2.82GB, Reserved: 3.08GB\n",
      "19:19:46 | INFO | AFTER EVAL Model - Training mode: True, Device: cuda:0\n",
      "19:19:46 | INFO | Evaluation completed successfully. Average loss: 1.3066\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING SUMMARY\n",
      "================================================================================\n",
      "+--------------+--------------+---------------+\n",
      "| Metric       |   Best Score | Achieved At   |\n",
      "+==============+==============+===============+\n",
      "| Best BLEU-4  |       0.0875 | Epoch 7       |\n",
      "+--------------+--------------+---------------+\n",
      "| Best ROUGE-L |       0.2762 | Epoch 7       |\n",
      "+--------------+--------------+---------------+\n",
      "| Best METEOR  |       0.3082 | Epoch 6       |\n",
      "+--------------+--------------+---------------+\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:20:08 | INFO | Saving Phase 2 model to ./t5-phase2-tuned\\final\n",
      "19:20:17 | INFO | Performing final evaluation...\n",
      "19:20:17 | INFO | === BEFORE EVALUATION ===\n",
      "19:20:17 | INFO | BEFORE EVAL GPU Memory - Allocated: 2.80GB, Reserved: 3.08GB\n",
      "19:20:17 | INFO | BEFORE EVAL Model - Training mode: True, Device: cuda:0\n",
      "19:20:24 | INFO | Starting evaluation on full validation set...\n",
      "Evaluating Epoch 8: 100%|██████████| 3696/3696 [1:38:46<00:00,  1.60s/batch]  \n",
      "20:59:11 | INFO | Computing metrics for 7392 predictions...\n",
      "20:59:11 | INFO | Computing quality and diversity metrics...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Adeptus_Mechanicus\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - Epoch 8\n",
      "================================================================================\n",
      "\n",
      "QUALITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| BLEU-1     |  0.259  |\n",
      "+------------+---------+\n",
      "| BLEU-2     |  0.1581 |\n",
      "+------------+---------+\n",
      "| BLEU-3     |  0.1127 |\n",
      "+------------+---------+\n",
      "| BLEU-4     |  0.0871 |\n",
      "+------------+---------+\n",
      "| ROUGE-L    |  0.2759 |\n",
      "+------------+---------+\n",
      "| METEOR     |  0.3067 |\n",
      "+------------+---------+\n",
      "| BERT-SCORE |  0.8871 |\n",
      "+------------+---------+\n",
      "| SELF-BLEU  |  0.9176 |\n",
      "+------------+---------+\n",
      "\n",
      "DIVERSITY METRICS:\n",
      "+------------+---------+\n",
      "| Metric     |   Score |\n",
      "+============+=========+\n",
      "| SELF-BLEU  |  0.9176 |\n",
      "+------------+---------+\n",
      "| DISTINCT-1 |  0.1205 |\n",
      "+------------+---------+\n",
      "| DISTINCT-2 |  0.3027 |\n",
      "+------------+---------+\n",
      "\n",
      "OTHER METRICS:\n",
      "+----------+---------+\n",
      "| Metric   |   Score |\n",
      "+==========+=========+\n",
      "| LOSS     |  1.3048 |\n",
      "+----------+---------+\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:34:34 | INFO | === AFTER EVALUATION ===\n",
      "21:34:34 | INFO | AFTER EVAL GPU Memory - Allocated: 2.82GB, Reserved: 3.08GB\n",
      "21:34:34 | INFO | AFTER EVAL Model - Training mode: True, Device: cuda:0\n",
      "21:34:34 | INFO | Evaluation completed successfully. Average loss: 1.3048\n",
      "21:34:34 | INFO | Generating sample predictions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "SAMPLE PREDICTIONS - 20 Context-Question Pairs\n",
      "====================================================================================================\n",
      "\n",
      "--- SAMPLE  1 ---\n",
      "CONTEXT: Last Thursday Kayla and I had appointments to get our hair done . We had two different goals in mind . Kayla wanted to get a perm and add some curl to her hair . Though she would n't let me take the c...\n",
      "ACTUAL:   What do we know about Kayla 's hair before she got her hair done ?\n",
      "PREDICTED: What may be a fact about this person ?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE  2 ---\n",
      "CONTEXT: Britain is facing a sharp rise in its rat population as growing numbers of people leave what they cannot finish of the fast food in the street, an environment group warned .Keep Britain Tidy said the ...\n",
      "ACTUAL:   What was the rat population in Britain in 2000 according to the writer?\n",
      "PREDICTED: How many people get Weil's Disease every year?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE  3 ---\n",
      "CONTEXT: Turtle Bay Resort: The Turtle Bay Resort is the major hotel on the North Shore of Oahu island in Hawaii.\n",
      "Turtle Bay Championship: The Turtle Bay Championship was a golf tournament on the Champions Tou...\n",
      "ACTUAL:   On which Hawaiian island is Turtle Bay Resort, home of the SBS Open women's golf tournament?\n",
      "PREDICTED: The SBS Open at Turtle Bay was a golf tournament for professional female golfers, played on the Palmer Course at which major hotel on which island in Hawaii?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE  4 ---\n",
      "CONTEXT: James the Turtle was always getting in trouble. Sometimes he'd reach into the freezer and empty out all the food. Other times he'd sled on the deck and get a splinter. His aunt Jane tried as hard as s...\n",
      "ACTUAL:   Where did James go after he went to the grocery store?\n",
      "PREDICTED: How many bags of fries did James order?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE  5 ---\n",
      "CONTEXT: There was no ring , no romance , no down on one knee - hell , it was n't even my dad that proposed . My mom just knew , and from experience , well , my mom does n't mess around . She just does it . I ...\n",
      "ACTUAL:   What kind of mother do I have ?\n",
      "PREDICTED: What may be a fact about this person 's situation ?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE  6 ---\n",
      "CONTEXT: California Boy: \"California Boy\" is a song by American rapper Lil B.  It was released for digital download on September 16, 2012, through Lil B's own record label, BasedWorld Records.  \"California Boy...\n",
      "ACTUAL:   What is the real name of the American rapper and record producer who made a guest appearance on the GO:OD AM album along with Ab-Soul, Lil B, Miguel and Little Dragon?\n",
      "PREDICTED: GO:OD AM features guest appearances from an American rapper and record producer from what city?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE  7 ---\n",
      "CONTEXT: I think the prospect of riding his bike again should do the trick . As if that were n't bad enough , he came down with some kind of 24-hour bug this past Friday that basically sent him to bed for the ...\n",
      "ACTUAL:   Why did he ask if Gatorade was light replacement ?\n",
      "PREDICTED: What may be a fact about this person 's situation ?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE  8 ---\n",
      "CONTEXT: Something bad happened to sam this morning. He fell over and broke his nose in the school hallway. When Sam looked up, he saw his friends. \"Are you OK?\" They asked him. But he didn't say anything to t...\n",
      "ACTUAL:   Why was Sam angry with his friends?\n",
      "PREDICTED: Why didn't Sam talk to his friends for the rest of the morning?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE  9 ---\n",
      "CONTEXT: Johann Friedrich Struensee: Johann Friedrich Struensee (5 August 1737 – 28 April 1772) was a German doctor.  He became royal physician to the mentally-ill King Christian VII of Denmark and a minister ...\n",
      "ACTUAL:   Schack Carl Rantzau is notable for his friendship with a physical to which king?\n",
      "PREDICTED: Schack Carl Rantzau is notable for his friendship with a German doctor who rose in power to what position?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE 10 ---\n",
      "CONTEXT: Jyoti Amge, an Indian girl, is the world's smallest woman. She is only 0.628 metres tall. She is small, but her dream is great. On her 18th birthday in 2012, she shared the good news with others. She ...\n",
      "ACTUAL:   How old is Jyoti Amge in 2013?\n",
      "PREDICTED: How tall is Jyoti Amge?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE 11 ---\n",
      "CONTEXT: Live at the Beacon Theater: Live at the Beacon Theater is the fourth full-length comedy special/concert film by comedian Louis C.K..  The special takes place at the Beacon Theatre in Manhattan, New Yo...\n",
      "ACTUAL:   Live at the Beacon Theater is the fourth full-length comedy special/concert film by comedian Louis C.K., a comedian, actor, writer, producer, director, and editor, is of which nationality?\n",
      "PREDICTED: Live at the Beacon Theater is the fourth full-length comedy special/concert film by comedian Louis C.K., an American comedian, actor, writer, producer, director, and editor, born on which date?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE 12 ---\n",
      "CONTEXT: The song was released as a digital download on 25 September 2015. It received mixed reviews from critics and fans, particularly in comparison to Adele's \"Skyfall\". The mixed reception to the song led ...\n",
      "ACTUAL:   What group wrote music for the film that ended up not being used?\n",
      "PREDICTED: What was the first Bond theme to reach number one in the UK Singles Chart?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE 13 ---\n",
      "CONTEXT: Death of Brian Deneke: On December 12, 1997, 19-year-old punk musician Brian Theodore Deneke (March 9, 1978 – December 12, 1997) was killed in a deliberate hit and run attack in Amarillo, Texas, by 17...\n",
      "ACTUAL:   What director is slated to direct the story of punk musician Brian Theodore Deneke?\n",
      "PREDICTED: Jamie Ryan Scott Brooks is slated to direct the true crime drama Bomb City based on the story of Brian Deneke, the 19-year-old punk musician who died in what year?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE 14 ---\n",
      "CONTEXT: After the formal end of Reconstruction, the struggle over power in Southern society continued. Through violence and intimidation against freedmen and their allies, White Democrats regained political p...\n",
      "ACTUAL:   In which year did the Tennessee General Assembly pass electoral reform laws that disenfranchised most African Americans in the state?\n",
      "PREDICTED: What was the percentage of African Americans in Tennessee in 1900?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE 15 ---\n",
      "CONTEXT: I was looking for a store that was open , besides Wal - Mart , to get my WIC around midnight . Tony and I had just come back from the dock but I decided to hurry up and get the formula since he was ac...\n",
      "ACTUAL:   What is probably true about the narrator ?\n",
      "PREDICTED: What may be a fact about this person ?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE 16 ---\n",
      "CONTEXT: Are you interested both in camp and music? Maybe the MSU Community Music School is a better place for you to go! It offers different music camps this summer!\n",
      "Rock Camp\n",
      "June 23 - 27, 2014\n",
      "$220\n",
      "It is he...\n",
      "ACTUAL:   What do the four camps have in common?\n",
      "PREDICTED: How much will the students pay if they go to the Rock Camp?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE 17 ---\n",
      "CONTEXT: The markets ended to the plus side on a snapback after a very nasty early gap lower . They stabilized and went on a 5-wave rally for the rest of the day . It was a rather choppy move .\n",
      "ACTUAL:   What are they talking about ?\n",
      "PREDICTED: What may be a fact about this person 's situation ?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE 18 ---\n",
      "CONTEXT: The canon law of the Eastern Catholic Churches, which had developed some different disciplines and practices, underwent its own process of codification, resulting in the Code of Canons of the Eastern ...\n",
      "ACTUAL:   Who sponsored the promulgation of Eastern Catholic Church laws?\n",
      "PREDICTED: In what year was the Code of Canons of the Eastern Churches promulgated?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- SAMPLE 19 ---\n",
      "CONTEXT: Robert Carey (gangster): Robert \"Bob\" Carey (August 25, 1894 – July 30, 1932) was a Midwestern armed robber and contract killer responsible for many crimes during the Prohibition era.  He is considere...\n",
      "ACTUAL:   What German-American Contract killer that was a member of Chicago's North Side Gange was a vitvim of the Saint Valentine's Day massacre?\n",
      "PREDICTED: What is the name of the charm used in the province of Padua, Italy that was used to appeal to Frank Gusenberg to heal the latter condition known for that reason as Saint Valentine's affliction?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:34:54 | INFO | Phase 2 pipeline completed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SAMPLE 20 ---\n",
      "CONTEXT: Daniel comes from Sydney. He is now staying in Beijing with his family. He usually gets up at about 6:30 am and has breakfast at 7:00 am. Then he leaves home at 7:15 am.\n",
      "He gets to school at 7:45 am. ...\n",
      "ACTUAL:   How many classes do they have a day?\n",
      "PREDICTED: How many lessons does Daniel have in the morning?\n",
      "--------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "PHASE 2 FINAL EVALUATION RESULTS:\n",
      "╒════════════╤═══════════════╕\n",
      "│ Metric     │   Final Score │\n",
      "╞════════════╪═══════════════╡\n",
      "│ EPOCH      │        8      │\n",
      "├────────────┼───────────────┤\n",
      "│ BERT_SCORE │        0.8871 │\n",
      "├────────────┼───────────────┤\n",
      "│ BLEU_1     │        0.259  │\n",
      "├────────────┼───────────────┤\n",
      "│ BLEU_2     │        0.1581 │\n",
      "├────────────┼───────────────┤\n",
      "│ BLEU_3     │        0.1127 │\n",
      "├────────────┼───────────────┤\n",
      "│ BLEU_4     │        0.0871 │\n",
      "├────────────┼───────────────┤\n",
      "│ DISTINCT_1 │        0.1205 │\n",
      "├────────────┼───────────────┤\n",
      "│ DISTINCT_2 │        0.3027 │\n",
      "├────────────┼───────────────┤\n",
      "│ LOSS       │        1.3048 │\n",
      "├────────────┼───────────────┤\n",
      "│ METEOR     │        0.3067 │\n",
      "├────────────┼───────────────┤\n",
      "│ ROUGE_L    │        0.2759 │\n",
      "├────────────┼───────────────┤\n",
      "│ SELF_BLEU  │        0.9176 │\n",
      "╘════════════╧═══════════════╛\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import transformers\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from transformers.trainer_utils import set_seed\n",
    "import evaluate\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import bert_score\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import warnings\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tabulate import tabulate\n",
    "from transformers.utils.logging import set_verbosity_error\n",
    "import gc\n",
    "import optuna  # --- NEW ---\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Setup logging for VS Code\n",
    "def setup_logging():\n",
    "    \"\"\"Setup logging configuration\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "        datefmt='%H:%M:%S',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(),\n",
    "            logging.FileHandler('training.log', encoding='utf-8')\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "#set_verbosity_error()\n",
    "\n",
    "# Ensure NLTK data is available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "class QuestionGenerationDataset(Dataset):\n",
    "    def __init__(self, contexts, questions, tokenizer, max_length=512):\n",
    "        self.contexts = contexts\n",
    "        self.questions = questions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.contexts[idx]\n",
    "        question = self.questions[idx]\n",
    "\n",
    "        input_text = f\"Generate a question from context: {context}\"\n",
    "\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        target_encoding = self.tokenizer(\n",
    "            question,\n",
    "            max_length=128,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encoding.input_ids.flatten(),\n",
    "            \"attention_mask\": input_encoding.attention_mask.flatten(),\n",
    "            \"labels\": target_encoding.input_ids.flatten()\n",
    "        }\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, data_dir, tokenizer):\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_merged_csv(self, file_path: str, split_name: str) -> Tuple[List[str], List[str]]:\n",
    "        contexts, questions = [], []\n",
    "        \n",
    "        # Counters for statistics\n",
    "        total_processed = 0\n",
    "        dropped_length = 0\n",
    "        dropped_empty = 0\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            logger.info(f\"Loading {split_name} data from: {file_path}\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='utf-8')\n",
    "                \n",
    "                if 'context' not in df.columns or 'question' not in df.columns:\n",
    "                    raise ValueError(f\"CSV must contain 'context' and 'question' columns. Found: {df.columns}\")\n",
    "\n",
    "                # Use tqdm to show progress bar since encoding checks can take a moment\n",
    "                for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Checking {split_name}\"):\n",
    "                    total_processed += 1\n",
    "                    \n",
    "                    # 1. Basic extraction and existence check\n",
    "                    if pd.notna(row.get('context', '')) and pd.notna(row.get('question', '')):\n",
    "                        context = str(row['context']).strip()\n",
    "                        question = str(row['question']).strip()\n",
    "                        \n",
    "                        if not context or not question:\n",
    "                            dropped_empty += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # 2. Check Token Length\n",
    "                        # We encode without special tokens first to get rough count, \n",
    "                        # or with them to be precise. 'truncation=False' ensures we get actual length.\n",
    "                        token_ids = self.tokenizer.encode(context, add_special_tokens=True)\n",
    "                        \n",
    "                        if len(token_ids) <= 512:\n",
    "                            contexts.append(context)\n",
    "                            questions.append(question)\n",
    "                        else:\n",
    "                            dropped_length += 1\n",
    "                    else:\n",
    "                        dropped_empty += 1\n",
    "\n",
    "                # --- LOGGING THE STATISTICS ---\n",
    "                logger.info(f\"--- DATA STATS FOR {split_name.upper()} ---\")\n",
    "                logger.info(f\"  Total Rows:      {total_processed}\")\n",
    "                logger.info(f\"  Kept:            {len(contexts)} ({len(contexts)/total_processed:.1%})\")\n",
    "                logger.info(f\"  Dropped (Empty): {dropped_empty}\")\n",
    "                logger.info(f\"  Dropped (>512):  {dropped_length} ({dropped_length/total_processed:.1%})\")\n",
    "                logger.info(f\"---------------------------------------\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading {file_path}: {e}\")\n",
    "                raise e\n",
    "        else:\n",
    "            logger.error(f\"File not found: {file_path}\")\n",
    "        \n",
    "        return contexts, questions\n",
    "\n",
    "    def load_phase2_datasets(self) -> Tuple[List[str], List[str], List[str], List[str]]:\n",
    "        \"\"\"\n",
    "        Loads merged_train.csv and merged_val.csv specifically for Phase 2\n",
    "        \"\"\"\n",
    "        # Construct paths\n",
    "        train_path = os.path.join(self.data_dir, \"merged_train.csv\")\n",
    "        val_path = os.path.join(self.data_dir, \"merged_val.csv\")\n",
    "\n",
    "        # Load Data\n",
    "        train_contexts, train_questions = self.load_merged_csv(train_path, \"Phase 2 Train\")\n",
    "        val_contexts, val_questions = self.load_merged_csv(val_path, \"Phase 2 Validation\")\n",
    "\n",
    "        # Shuffle logic\n",
    "        train_combined = list(zip(train_contexts, train_questions))\n",
    "        val_combined = list(zip(val_contexts, val_questions))\n",
    "        \n",
    "        random.seed(42) \n",
    "        random.shuffle(train_combined)\n",
    "        random.shuffle(val_combined)\n",
    "\n",
    "        if train_combined:\n",
    "            train_contexts, train_questions = zip(*train_combined)\n",
    "        if val_combined:\n",
    "            val_contexts, val_questions = zip(*val_combined)\n",
    "\n",
    "        return (list(train_contexts), list(train_questions),\n",
    "                list(val_contexts), list(val_questions))\n",
    "\n",
    "class AdvancedEvaluationMetrics:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        self.smoothing = SmoothingFunction().method1\n",
    "\n",
    "    def compute_bleu(self, references, predictions):\n",
    "        bleu_scores = {f\"bleu_{i}\": [] for i in range(1, 5)}\n",
    "        for ref, pred in zip(references, predictions):\n",
    "            ref_tokens, pred_tokens = ref.split(), pred.split()\n",
    "            for i in range(1, 5):\n",
    "                weights = [1/i] * i\n",
    "                score = sentence_bleu([ref_tokens], pred_tokens, weights=weights, smoothing_function=self.smoothing)\n",
    "                bleu_scores[f\"bleu_{i}\"].append(score)\n",
    "        return {k: np.mean(v) for k, v in bleu_scores.items()}\n",
    "\n",
    "    def compute_rouge_l(self, references, predictions):\n",
    "        scores = [self.rouge_scorer.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(references, predictions)]\n",
    "        return {\"rouge_l\": np.mean(scores)}\n",
    "\n",
    "    def compute_meteor(self, references, predictions):\n",
    "        try:\n",
    "            meteor = evaluate.load(\"meteor\")\n",
    "            return {\"meteor\": meteor.compute(predictions=predictions, references=references)[\"meteor\"]}\n",
    "        except Exception:\n",
    "            return {\"meteor\": 0.0}\n",
    "\n",
    "    def compute_bert_score(self, references, predictions):\n",
    "        try:\n",
    "            P, R, F1 = bert_score.score(predictions, references, lang=\"en\", verbose=False)\n",
    "            return {\"bert_score\": F1.mean().item()}\n",
    "        except Exception:\n",
    "            return {\"bert_score\": 0.0}\n",
    "\n",
    "    # --- MODIFICATION 1: SELF-BLEU FIXED ---\n",
    "    def compute_self_bleu(self, predictions):\n",
    "        if len(predictions) < 2: return {\"self_bleu\": 0.0}\n",
    "        scores = []\n",
    "        for i, pred in enumerate(predictions):\n",
    "            others = predictions[:i] + predictions[i+1:]\n",
    "            pred_tokens = pred.split()\n",
    "            # The fix: Using all other sentences, not just the first 10\n",
    "            other_tokens = [other.split() for other in others]\n",
    "            if other_tokens:\n",
    "                score = sentence_bleu(other_tokens, pred_tokens, smoothing_function=self.smoothing)\n",
    "                scores.append(score)\n",
    "        return {\"self_bleu\": np.mean(scores) if scores else 0.0}\n",
    "\n",
    "    def compute_distinct_n(self, predictions, n):\n",
    "        all_ngrams = [tuple(tokens[i:i+n]) for pred in predictions for tokens in [pred.split()] for i in range(len(tokens)-n+1)]\n",
    "        if not all_ngrams: return 0.0\n",
    "        return len(set(all_ngrams)) / len(all_ngrams)\n",
    "\n",
    "    def compute_all_metrics(self, references, predictions):\n",
    "        if not references or not predictions:\n",
    "            return {m: 0.0 for m in [\"bleu_1\", \"bleu_2\", \"bleu_3\", \"bleu_4\", \"rouge_l\", \"meteor\", \"bert_score\", \"self_bleu\", \"distinct_1\", \"distinct_2\"]}\n",
    "        \n",
    "        metrics = {}\n",
    "        logger.info(\"Computing quality and diversity metrics...\")\n",
    "        metrics.update(self.compute_bleu(references, predictions))\n",
    "        metrics.update(self.compute_rouge_l(references, predictions))\n",
    "        metrics.update(self.compute_meteor(references, predictions))\n",
    "        metrics.update(self.compute_bert_score(references, predictions))\n",
    "        metrics.update(self.compute_self_bleu(predictions))\n",
    "        metrics[\"distinct_1\"] = self.compute_distinct_n(predictions, 1)\n",
    "        metrics[\"distinct_2\"] = self.compute_distinct_n(predictions, 2)\n",
    "        return metrics\n",
    "\n",
    "class DiverseDecoder:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def diverse_generate(self, input_ids, attention_mask, num_return_sequences=1):\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=128,\n",
    "            num_beams=8,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beam_groups=4,\n",
    "            diversity_penalty=1.5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "class MetricsLogger:\n",
    "    def __init__(self):\n",
    "        self.evaluation_history = []\n",
    "\n",
    "    def log_epoch_progress(self, epoch, loss, learning_rate):\n",
    "        logger.info(f\"Epoch {epoch:>.2f} | Loss: {loss:.4f} | LR: {learning_rate:.2e}\")\n",
    "\n",
    "    def log_evaluation(self, metrics, epoch=None, step=None):\n",
    "        eval_record = {'epoch': epoch, 'step': step, 'timestamp': datetime.now(), **metrics}\n",
    "        self.evaluation_history.append(eval_record)\n",
    "        self.display_metrics_table(metrics, epoch)\n",
    "\n",
    "    def display_metrics_table(self, metrics, epoch=None, step=None):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"EVALUATION RESULTS - Epoch {epoch}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        quality = {k: v for k, v in metrics.items() if any(x in k for x in ['bleu', 'rouge', 'meteor', 'bert_score'])}\n",
    "        diversity = {k: v for k, v in metrics.items() if any(x in k for x in ['self_bleu', 'distinct'])}\n",
    "        other = {k: v for k, v in metrics.items() if k not in quality and k not in diversity}\n",
    "        \n",
    "        for title, metric_dict in [(\"QUALITY\", quality), (\"DIVERSITY\", diversity), (\"OTHER\", other)]:\n",
    "            if metric_dict:\n",
    "                print(f\"\\n{title} METRICS:\")\n",
    "                table_data = [[k.replace('eval_', '').replace('_', '-').upper(), f\"{v:.4f}\"] for k, v in metric_dict.items()]\n",
    "                print(tabulate(table_data, headers=['Metric', 'Score'], tablefmt='grid'))\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    def display_training_summary(self):\n",
    "        if not self.evaluation_history: return\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TRAINING SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        eval_logs = [log for log in self.evaluation_history if 'eval_bleu_4' in log]\n",
    "        if not eval_logs: return\n",
    "        \n",
    "        best_bleu4 = max(eval_logs, key=lambda x: x.get('eval_bleu_4', 0))\n",
    "        best_rouge = max(eval_logs, key=lambda x: x.get('eval_rouge_l', 0))\n",
    "        best_meteor = max(eval_logs, key=lambda x: x.get('eval_meteor', 0))\n",
    "        \n",
    "        summary_data = [\n",
    "            ['Best BLEU-4', f\"{best_bleu4.get('eval_bleu_4', 0):.4f}\", f\"Epoch {best_bleu4.get('epoch', 'N/A')}\"],\n",
    "            ['Best ROUGE-L', f\"{best_rouge.get('eval_rouge_l', 0):.4f}\", f\"Epoch {best_rouge.get('epoch', 'N/A')}\"],\n",
    "            ['Best METEOR', f\"{best_meteor.get('eval_meteor', 0):.4f}\", f\"Epoch {best_meteor.get('epoch', 'N/A')}\"],\n",
    "        ]\n",
    "        print(tabulate(summary_data, headers=['Metric', 'Best Score', 'Achieved At'], tablefmt='grid'))\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "class CustomLoggingCallback(TrainerCallback):\n",
    "    def __init__(self, metrics_logger=None):\n",
    "        self.metrics_logger = metrics_logger if metrics_logger is not None else MetricsLogger()\n",
    "\n",
    "    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        if state.is_world_process_zero and logs:\n",
    "            eval_metrics = {k: v for k, v in logs.items() if k.startswith(\"eval_\") and isinstance(v, (int, float))}\n",
    "            if eval_metrics:\n",
    "                self.metrics_logger.log_evaluation(\n",
    "                    metrics=eval_metrics,\n",
    "                    epoch=int(logs.get('epoch', state.epoch or 0)),\n",
    "                    step=state.global_step\n",
    "                )\n",
    "            elif any(key in logs for key in ['train_loss', 'loss']) and 'learning_rate' in logs:\n",
    "                loss = logs.get('train_loss', logs.get('loss', 0))\n",
    "                self.metrics_logger.log_epoch_progress(\n",
    "                    epoch=logs.get('epoch', state.epoch or 0),\n",
    "                    loss=loss,\n",
    "                    learning_rate=logs['learning_rate']\n",
    "                )\n",
    "\n",
    "    def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.is_world_process_zero:\n",
    "            self.metrics_logger.display_training_summary()\n",
    "\n",
    "# --- MODIFICATION 2, PART 1: ADD NEW CALLBACK CLASS ---\n",
    "class AverageTrainLossLogger(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.epoch_train_losses = []\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Reset the list of losses at the start of each epoch.\"\"\"\n",
    "        self.epoch_train_losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"On each log step, if it's a training log, append the loss.\"\"\"\n",
    "        if 'loss' in logs and 'learning_rate' in logs:\n",
    "            self.epoch_train_losses.append(logs['loss'])\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"At the end of the epoch, calculate and log the average.\"\"\"\n",
    "        if self.epoch_train_losses:\n",
    "            avg_epoch_loss = np.mean(self.epoch_train_losses)\n",
    "            logger.info(f\"===== Average Training Loss for Epoch {int(state.epoch)}: {avg_epoch_loss:.4f} =====\")\n",
    "\n",
    "# --- NEW: CALLBACK FOR OPTUNA PRUNING ---\n",
    "class OptunaPruningCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A TrainerCallback to report evaluation metrics to Optuna for pruning.\n",
    "    \"\"\"\n",
    "    def __init__(self, trial: optuna.Trial):\n",
    "        self.trial = trial\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics: dict, **kwargs):\n",
    "        # We report the metric we're optimizing for (bleu_4)\n",
    "        if \"eval_bleu_4\" in metrics:\n",
    "            metric_value = metrics[\"eval_bleu_4\"]\n",
    "            self.trial.report(metric_value, state.global_step)\n",
    "            \n",
    "            # Check if the trial should be pruned\n",
    "            if self.trial.should_prune():\n",
    "                logger.warning(f\"Trial {self.trial.number} pruned at step {state.global_step} with BLEU-4: {metric_value}.\")\n",
    "                raise optuna.TrialPruned()\n",
    "        elif \"eval_loss\" in metrics:\n",
    "            # Fallback to loss if bleu_4 isn't available for some reason\n",
    "            self.trial.report(metrics[\"eval_loss\"], state.global_step)\n",
    "            if self.trial.should_prune():\n",
    "                logger.warning(f\"Trial {self.trial.number} pruned at step {state.global_step} with Loss: {metrics['eval_loss']}.\")\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "def log_gpu_memory(prefix=\"\"):\n",
    "    \"\"\"Log current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        logger.info(f\"{prefix} GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "    else:\n",
    "        logger.info(f\"{prefix} GPU not available\")\n",
    "\n",
    "def log_model_state(model, prefix=\"\"):\n",
    "    \"\"\"Log current model state\"\"\"\n",
    "    logger.info(f\"{prefix} Model - Training mode: {model.training}, Device: {next(model.parameters()).device}\")\n",
    "\n",
    "# --- MODIFIED: CustomTrainer now accepts an `is_tuning_trial` flag ---\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, evaluator, metrics_logger=None, is_tuning_trial=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.evaluator = evaluator\n",
    "        self.decoder = DiverseDecoder(self.model, self.processing_class)\n",
    "        self.metrics_logger = metrics_logger or MetricsLogger()\n",
    "        self.is_tuning_trial = is_tuning_trial  # <-- Store the flag\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        # --- MODIFIED: Check if we are in a tuning trial ---\n",
    "        if self.is_tuning_trial:\n",
    "            logger.info(f\"=== TUNING TRIAL EVALUATION (BLEU-4 Only) ===\")\n",
    "        else:\n",
    "            logger.info(\"=== BEFORE EVALUATION ===\")\n",
    "            log_gpu_memory(\"BEFORE EVAL\")\n",
    "            log_model_state(self.model, \"BEFORE EVAL\")\n",
    "        \n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        self.model.eval()\n",
    "        \n",
    "        predictions, references = [], []\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        epoch = self.state.epoch if self.state.epoch is not None else 0\n",
    "        if not self.is_tuning_trial:\n",
    "             logger.info(\"Starting evaluation on full validation set...\")\n",
    "        \n",
    "        eval_desc = f\"Evaluating Epoch {int(epoch)}\"\n",
    "        if self.is_tuning_trial:\n",
    "            eval_desc = f\"Tuning Trial {self.state.trial_number} Eval\" if hasattr(self.state, 'trial_number') else \"Tuning Trial Eval\"\n",
    "            \n",
    "        eval_progress = tqdm(eval_dataloader, desc=eval_desc, unit=\"batch\", disable=False) # Disable tqdm in tuning\n",
    "\n",
    "        try:\n",
    "            for batch_idx, batch in enumerate(eval_progress):\n",
    "                batch_device = {k: v.to(self.args.device) for k, v in batch.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**batch_device)\n",
    "                    total_loss += outputs.loss.item()\n",
    "                    num_batches += 1\n",
    "                    \n",
    "                    inputs = {k: v for k, v in batch_device.items() if k != \"labels\"}\n",
    "                    generated_ids = self.decoder.diverse_generate(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "                    batch_predictions = self.processing_class.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                    predictions.extend(batch_predictions)\n",
    "                    \n",
    "                    labels = batch[\"labels\"].cpu().numpy()\n",
    "                    labels = np.where(labels != -100, labels, self.processing_class.pad_token_id)\n",
    "                    batch_references = self.processing_class.batch_decode(labels, skip_special_tokens=True)\n",
    "                    references.extend(batch_references)\n",
    "                    \n",
    "                    if (batch_idx + 1) % 20 == 0:\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during evaluation: {e}\")\n",
    "            return {f\"{metric_key_prefix}_loss\": float('inf')}\n",
    "\n",
    "        # --- MODIFIED: Conditional metric computation ---\n",
    "        metrics = {}\n",
    "        if self.is_tuning_trial:\n",
    "            # For tuning, ONLY compute loss and BLEU-4\n",
    "            if not self.is_tuning_trial: logger.info(f\"Tuning trial: Computing only BLEU-4 for {len(predictions)} predictions...\")\n",
    "            bleu_scores = self.evaluator.compute_bleu(references, predictions)\n",
    "            metrics = {\n",
    "                f\"{metric_key_prefix}_bleu_4\": bleu_scores.get('bleu_4', 0.0)\n",
    "            }\n",
    "        else:\n",
    "            # Full evaluation for the final run\n",
    "            logger.info(f\"Computing metrics for {len(predictions)} predictions...\")\n",
    "            metrics = self.evaluator.compute_all_metrics(references, predictions)\n",
    "            metrics = {f\"{metric_key_prefix}_{k}\": v for k, v in metrics.items()}\n",
    "        # --- END OF MODIFICATION ---\n",
    "        \n",
    "        avg_loss = total_loss / max(num_batches, 1)\n",
    "        metrics[f\"{metric_key_prefix}_loss\"] = avg_loss\n",
    "        \n",
    "        self.log(metrics)\n",
    "        \n",
    "        del predictions, references, batch_device\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        if not self.is_tuning_trial:\n",
    "            logger.info(\"=== AFTER EVALUATION ===\")\n",
    "            log_gpu_memory(\"AFTER EVAL\")\n",
    "            log_model_state(self.model, \"AFTER EVAL\")\n",
    "            logger.info(f\"Evaluation completed successfully. Average loss: {avg_loss:.4f}\")\n",
    "            \n",
    "        return metrics\n",
    "\n",
    "def generate_sample_predictions(model, tokenizer, eval_contexts, eval_questions, num_samples=20):\n",
    "    logger.info(\"Generating sample predictions...\")\n",
    "    indices = random.sample(range(len(eval_contexts)), min(num_samples, len(eval_contexts)))\n",
    "    model.eval()\n",
    "    decoder = DiverseDecoder(model, tokenizer)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"SAMPLE PREDICTIONS - 20 Context-Question Pairs\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for i, idx in enumerate(indices, 1):\n",
    "        context, actual_question = eval_contexts[idx], eval_questions[idx]\n",
    "        input_text = f\"Generate a question from context: {context}\"\n",
    "        input_encoding = tokenizer(input_text, max_length=512, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = decoder.diverse_generate(input_encoding[\"input_ids\"], input_encoding[\"attention_mask\"])\n",
    "            predicted_question = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"\\n--- SAMPLE {i:2d} ---\")\n",
    "        print(f\"CONTEXT: {context[:200]}{'...' if len(context) > 200 else ''}\")\n",
    "        print(f\"ACTUAL:   {actual_question}\")\n",
    "        print(f\"PREDICTED: {predicted_question}\")\n",
    "        print(\"-\" * 80)\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "def setup_training_args(output_dir, num_train_epochs=5, train_dataset_size=0):\n",
    "    effective_batch_size = 8 * 2\n",
    "    steps_per_epoch = max(1, train_dataset_size // effective_batch_size)\n",
    "    \n",
    "    logger.info(f\"Training Configuration: Dataset size: {train_dataset_size:,}, Effective batch size: {effective_batch_size}, Steps per epoch: {steps_per_epoch}\")\n",
    "    \n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        torch_compile=True,\n",
    "        learning_rate=5e-5,  # This will be overridden by Optuna\n",
    "        weight_decay=0.01, # This will be overridden by Optuna\n",
    "        warmup_steps=min(500, steps_per_epoch), # This will be overridden by Optuna\n",
    "        logging_steps=max(10, steps_per_epoch // 10),\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_bleu_4\",\n",
    "        greater_is_better=True,\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=0,\n",
    "        remove_unused_columns=False,\n",
    "        gradient_checkpointing=True,\n",
    "        lr_scheduler_type=\"cosine\", # This will be overridden by Optuna\n",
    "        save_total_limit=3, # Will use 1 for tuning\n",
    "        report_to=\"none\",\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "        group_by_length=True,\n",
    "    )\n",
    "\n",
    "# --- CONFIGURATION FOR PHASE 2 ---\n",
    "# REPLACE THIS PATH with the actual folder where Phase 1 saved the 'final' model\n",
    "# Example: \"./t5-flan-question-generation-tuned/final\" or \"E:/Models/Phase1/final\"\n",
    "g_model_name = \"./t5-flan-question-generation-tuned/final\" \n",
    "\n",
    "# Global variables (same as before)\n",
    "g_tokenizer = None\n",
    "g_train_dataset = None\n",
    "g_eval_dataset = None\n",
    "g_data_collator = None\n",
    "g_evaluator = None\n",
    "g_eval_contexts = None\n",
    "g_eval_questions = None\n",
    "\n",
    "# --- NEW: Optuna Objective Function ---\n",
    "def objective(trial: optuna.Trial):\n",
    "    global g_tokenizer, g_model_name, g_train_dataset, g_eval_dataset, g_data_collator, g_evaluator\n",
    "    \n",
    "    logger.info(f\"--- Starting Optuna Trial {trial.number} ---\")\n",
    "    \n",
    "    # --- 1. Define Search Space ---\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"constant\"])\n",
    "    \n",
    "    # Calculate steps_per_epoch for warmup suggestion\n",
    "    effective_batch_size = 8 * 2\n",
    "    steps_per_epoch = max(1, len(g_train_dataset) // effective_batch_size)\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 100, steps_per_epoch) \n",
    "    \n",
    "    # --- USE SMALL EVAL SUBSET FOR TUNING (e.g., 10% or 5000 samples max) ---\n",
    "    eval_subset_size = min(5000, int(len(g_eval_dataset) * 0.3))  # Use 10% of eval data, max 5000\n",
    "    eval_indices = random.sample(range(len(g_eval_dataset)), eval_subset_size)\n",
    "    eval_subset = torch.utils.data.Subset(g_eval_dataset, eval_indices)\n",
    "    \n",
    "    logger.info(f\"Trial {trial.number}: Using {len(g_train_dataset)} train samples and {eval_subset_size} eval samples (subset for speed)\")\n",
    "    \n",
    "    # --- 2. Configure Training ---\n",
    "    model = T5ForConditionalGeneration.from_pretrained(g_model_name)\n",
    "    if g_tokenizer.pad_token is not None and g_tokenizer.pad_token_id > g_tokenizer.vocab_size:\n",
    "         model.resize_token_embeddings(len(g_tokenizer))\n",
    "    \n",
    "    output_dir = f\"./optuna-trials/trial_{trial.number}\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=8,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        torch_compile=True,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        logging_steps=max(10, steps_per_epoch // 10),\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_bleu_4\",\n",
    "        greater_is_better=True,\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=0,\n",
    "        remove_unused_columns=False,\n",
    "        gradient_checkpointing=True,\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "        group_by_length=True,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "    )\n",
    "    \n",
    "    pruning_callback = OptunaPruningCallback(trial)\n",
    "    trial_data_collator = DataCollatorForSeq2Seq(tokenizer=g_tokenizer, model=model, padding=True, max_length=512, label_pad_token_id=-100)\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        evaluator=g_evaluator,\n",
    "        metrics_logger=None,\n",
    "        is_tuning_trial=True,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=g_train_dataset,  # Full training data\n",
    "        eval_dataset=eval_subset,        # <--- SUBSET for fast evaluation\n",
    "        tokenizer=g_tokenizer,\n",
    "        data_collator=trial_data_collator,\n",
    "        callbacks=[\n",
    "            pruning_callback,\n",
    "            EarlyStoppingCallback(early_stopping_patience=4)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    trainer.state.trial_number = trial.number\n",
    "    \n",
    "    # --- 3. Train ---\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except optuna.TrialPruned:\n",
    "        logger.info(f\"Trial {trial.number} was pruned.\")\n",
    "        del model, trainer, trial_data_collator\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in trial {trial.number}: {e}\")\n",
    "        del model, trainer, trial_data_collator\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return 0.0\n",
    "\n",
    "    # --- 4. Report & Return ---\n",
    "    best_metric = trainer.state.best_metric\n",
    "    logger.info(f\"--- Finished Optuna Trial {trial.number} | Best BLEU-4: {best_metric} ---\")\n",
    "    \n",
    "    del model, trainer, trial_data_collator\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return best_metric if best_metric is not None else 0.0\n",
    "\n",
    "\n",
    "# --- MODIFIED: Main function now orchestrates setup, tuning, and final training ---\n",
    "def main():\n",
    "    global g_tokenizer, g_model_name, g_train_dataset, g_eval_dataset, g_data_collator, g_evaluator, g_eval_contexts, g_eval_questions\n",
    "    \n",
    "    set_seed(42)\n",
    "    logger.info(\"Starting PHASE 2: T5 Question Generation Training\")\n",
    "    \n",
    "    # --- CONFIGURATION ---\n",
    "    data_dir = \"E:/A_CSE499/data\"  # Directory containing merged_train.csv and merged_val.csv\n",
    "    output_dir = \"./t5-phase2-tuned\" # New output directory for Phase 2 results\n",
    "    \n",
    "    # --- 1. SETUP ---\n",
    "    # IMPORTANT: This loads the model and tokenizer from your Phase 1 local directory\n",
    "    logger.info(f\"Loading Phase 1 model from: {g_model_name}...\")\n",
    "    try:\n",
    "        g_tokenizer = T5Tokenizer.from_pretrained(g_model_name)\n",
    "        \n",
    "        # We load the model here to verify it loads correctly, then release it\n",
    "        # It will be re-loaded inside the Optuna objective/Final training\n",
    "        dummy_model = T5ForConditionalGeneration.from_pretrained(g_model_name)\n",
    "        \n",
    "        # Ensure special tokens are handled if they were saved with the tokenizer\n",
    "        if g_tokenizer.pad_token is None:\n",
    "            g_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            dummy_model.resize_token_embeddings(len(g_tokenizer))\n",
    "            \n",
    "    except OSError as e:\n",
    "        logger.error(f\"Could not load Phase 1 model from {g_model_name}. Check the path.\")\n",
    "        logger.error(str(e))\n",
    "        return\n",
    "\n",
    "    # Load Phase 2 Data\n",
    "    logger.info(\"Loading Phase 2 (Merged) datasets...\")\n",
    "    data_processor = DataProcessor(data_dir, g_tokenizer)\n",
    "    \n",
    "    # CALLING THE NEW DATA LOADER\n",
    "    train_contexts, train_questions, g_eval_contexts, g_eval_questions = data_processor.load_phase2_datasets()\n",
    "    \n",
    "    if not train_contexts:\n",
    "        logger.error(\"No training data loaded! Please check merged_train.csv path.\")\n",
    "        return\n",
    "        \n",
    "    g_train_dataset = QuestionGenerationDataset(train_contexts, train_questions, g_tokenizer)\n",
    "    g_eval_dataset = QuestionGenerationDataset(g_eval_contexts, g_eval_questions, g_tokenizer)\n",
    "    \n",
    "    # Setup Collator (using dummy model for config)\n",
    "    g_data_collator = DataCollatorForSeq2Seq(tokenizer=g_tokenizer, model=dummy_model, padding=True, max_length=512, label_pad_token_id=-100)\n",
    "    del dummy_model\n",
    "    \n",
    "    g_evaluator = AdvancedEvaluationMetrics(g_tokenizer)\n",
    "    \n",
    "    # --- 2. OPTUNA TUNING (Optional for Phase 2, but recommended) ---\n",
    "    logger.info(\"=== STARTING PHASE 2 HYPERPARAMETER TUNING ===\")\n",
    "    \n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=3)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name=\"t5-phase2-tuning\", # Changed name\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        storage=\"sqlite:///t5_phase2_tuning.db\", # Changed DB file\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    n_trials = 10\n",
    "    n_completed_trials = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])\n",
    "    \n",
    "    if n_completed_trials >= n_trials:\n",
    "        logger.info(f\"Study already has {n_completed_trials} completed trials. Skipping optimization.\")\n",
    "    else:\n",
    "        n_remaining_trials = n_trials - n_completed_trials\n",
    "        logger.info(f\"Resuming study. {n_completed_trials} trials complete, running {n_remaining_trials} more.\")\n",
    "        try:\n",
    "            study.optimize(objective, n_trials=n_remaining_trials)\n",
    "        except KeyboardInterrupt:\n",
    "            logger.warning(\"Optuna optimization interrupted.\")\n",
    "    \n",
    "    logger.info(\"=== TUNING COMPLETE ===\")\n",
    "    best_trial = study.best_trial\n",
    "    logger.info(f\"Best parameters: {json.dumps(best_trial.params, indent=2)}\")\n",
    "\n",
    "    # --- 3. FINAL PHASE 2 TRAINING ---\n",
    "    logger.info(\"=== STARTING PHASE 2 FINAL TRAINING ===\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Load the Phase 1 model again for final training\n",
    "    model = T5ForConditionalGeneration.from_pretrained(g_model_name)\n",
    "    if g_tokenizer.pad_token_id > model.config.vocab_size:\n",
    "         model.resize_token_embeddings(len(g_tokenizer))\n",
    "         \n",
    "    shared_metrics_logger = MetricsLogger()\n",
    "    custom_logging_callback = CustomLoggingCallback(shared_metrics_logger)\n",
    "    avg_loss_callback = AverageTrainLossLogger()\n",
    "    \n",
    "    best_params = best_trial.params\n",
    "    \n",
    "    training_args = setup_training_args(\n",
    "        output_dir,\n",
    "        num_train_epochs=8, \n",
    "        train_dataset_size=len(g_train_dataset)\n",
    "    )\n",
    "    \n",
    "    training_args.learning_rate = best_params[\"learning_rate\"]\n",
    "    training_args.weight_decay = best_params[\"weight_decay\"]\n",
    "    training_args.warmup_steps = best_params[\"warmup_steps\"]\n",
    "    training_args.lr_scheduler_type = best_params[\"lr_scheduler_type\"]\n",
    "    \n",
    "    final_data_collator = DataCollatorForSeq2Seq(tokenizer=g_tokenizer, model=model, padding=True, max_length=512, label_pad_token_id=-100)\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        evaluator=g_evaluator,\n",
    "        metrics_logger=shared_metrics_logger,\n",
    "        is_tuning_trial=False,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=g_train_dataset,\n",
    "        eval_dataset=g_eval_dataset,\n",
    "        tokenizer=g_tokenizer,\n",
    "        data_collator=final_data_collator,\n",
    "        callbacks=[\n",
    "            custom_logging_callback,\n",
    "            avg_loss_callback,\n",
    "            EarlyStoppingCallback(early_stopping_patience=4)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Starting Phase 2 training...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"Training interrupted. Saving current state...\")\n",
    "        trainer.save_model(os.path.join(output_dir, \"interrupted\"))\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Saving Phase 2 model to {os.path.join(output_dir, 'final')}\")\n",
    "    trainer.save_model(os.path.join(output_dir, \"final\"))\n",
    "    g_tokenizer.save_pretrained(os.path.join(output_dir, \"final\"))\n",
    "    \n",
    "    logger.info(\"Performing final evaluation...\")\n",
    "    final_metrics = trainer.evaluate()\n",
    "    \n",
    "    generate_sample_predictions(model, g_tokenizer, g_eval_contexts, g_eval_questions)\n",
    "    \n",
    "    # Print Final Table\n",
    "    final_results = [[k.replace('eval_', '').upper(), f\"{v:.4f}\"] for k, v in sorted(final_metrics.items()) if isinstance(v, (int, float))]\n",
    "    if final_results:\n",
    "        print(\"\\nPHASE 2 FINAL EVALUATION RESULTS:\")\n",
    "        print(tabulate(final_results, headers=['Metric', 'Final Score'], tablefmt='fancy_grid'))\n",
    "    \n",
    "    logger.info(\"Phase 2 pipeline completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
