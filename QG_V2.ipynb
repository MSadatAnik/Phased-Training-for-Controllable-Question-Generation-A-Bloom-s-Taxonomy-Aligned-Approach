{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMNhcf+VrMwVE7AE7VXmC8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "554620cab13c4c59b61120ce243537f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9afe2bc392374f5cbae77caeceadd5ea",
              "IPY_MODEL_ae41b1a6c31845c78ebfe614c81280af",
              "IPY_MODEL_5cc380403abf4befb6d6d2bdefe36584"
            ],
            "layout": "IPY_MODEL_40a873165e254f6eb68d8430c9fa7c1f"
          }
        },
        "9afe2bc392374f5cbae77caeceadd5ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_880877cb8bf14f49a3d6218d488f3b51",
            "placeholder": "​",
            "style": "IPY_MODEL_b5b9c0d198f34d7e8a91e19a12e72b38",
            "value": ""
          }
        },
        "ae41b1a6c31845c78ebfe614c81280af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d149f4711c5e4aadaa9fc743c378bca7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38daa98113ce4803959f5c062f8e6307",
            "value": 0
          }
        },
        "5cc380403abf4befb6d6d2bdefe36584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f195ec9962e4db8adb7d833d26d2f12",
            "placeholder": "​",
            "style": "IPY_MODEL_9b3873b15de44e92a07a7701c9d3af8f",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "40a873165e254f6eb68d8430c9fa7c1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "880877cb8bf14f49a3d6218d488f3b51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5b9c0d198f34d7e8a91e19a12e72b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d149f4711c5e4aadaa9fc743c378bca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "38daa98113ce4803959f5c062f8e6307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f195ec9962e4db8adb7d833d26d2f12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b3873b15de44e92a07a7701c9d3af8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSadatAnik/QuestionGeneration/blob/Sadat_Branch/QG_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 1\n",
        "!pip install transformers==4.48.0 datasets nltk rouge_score sacrebleu==2.4.3 meteor\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7dm4-WzslvPg",
        "outputId": "7843f886-763f-4e5f-83c0-75da4eff04ec",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.48.0\n",
            "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: sacrebleu==2.4.3 in /usr/local/lib/python3.11/dist-packages (2.4.3)\n",
            "Requirement already satisfied: meteor in /usr/local/lib/python3.11/dist-packages (2.0.18)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (4.67.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu==2.4.3) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu==2.4.3) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu==2.4.3) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu==2.4.3) (5.4.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (15.0.2)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: bgzip<0.6.0,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from meteor) (0.5.0)\n",
            "Requirement already satisfied: biom-format<3.0.0,>=2.1.15 in /usr/local/lib/python3.11/dist-packages (from meteor) (2.1.16)\n",
            "Requirement already satisfied: cogent3<2025.0.0,>=2024.2.5a1 in /usr/local/lib/python3.11/dist-packages (from meteor) (2024.12.19a2)\n",
            "Requirement already satisfied: ete3<4.0.0,>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from meteor) (3.1.3)\n",
            "Requirement already satisfied: pysam<0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from meteor) (0.22.1)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from biom-format<3.0.0,>=2.1.15->meteor) (1.15.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from biom-format<3.0.0,>=2.1.15->meteor) (3.14.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from cogent3<2025.0.0,>=2024.2.5a1->meteor) (5.2.0)\n",
            "Requirement already satisfied: loky in /usr/local/lib/python3.11/dist-packages (from cogent3<2025.0.0,>=2024.2.5a1->meteor) (3.5.5)\n",
            "Requirement already satisfied: numba>0.54 in /usr/local/lib/python3.11/dist-packages (from cogent3<2025.0.0,>=2024.2.5a1->meteor) (0.60.0)\n",
            "Requirement already satisfied: scitrack in /usr/local/lib/python3.11/dist-packages (from cogent3<2025.0.0,>=2024.2.5a1->meteor) (2024.10.8)\n",
            "Requirement already satisfied: stevedore in /usr/local/lib/python3.11/dist-packages (from cogent3<2025.0.0,>=2024.2.5a1->meteor) (5.4.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from cogent3<2025.0.0,>=2024.2.5a1->meteor) (4.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.0) (2025.6.15)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>0.54->cogent3<2025.0.0,>=2024.2.5a1->meteor) (0.43.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from loky->cogent3<2025.0.0,>=2024.2.5a1->meteor) (3.1.1)\n",
            "Requirement already satisfied: pbr>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from stevedore->cogent3<2025.0.0,>=2024.2.5a1->meteor) (6.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pbr>=2.0.0->stevedore->cogent3<2025.0.0,>=2024.2.5a1->meteor) (75.2.0)\n",
            "Downloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.0\n",
            "    Uninstalling transformers-4.53.0:\n",
            "      Successfully uninstalled transformers-4.53.0\n",
            "Successfully installed transformers-4.48.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "f467c7ddda104d5fb3515b7cc9f3ebae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 2\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from itertools import chain\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq, HfArgumentParser,\n",
        "    set_seed, AutoConfig, EarlyStoppingCallback,\n",
        ")\n",
        "from datasets import Dataset as HFDataset\n",
        "from datasets import load_metric\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "pV_tRLScwNxY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "554620cab13c4c59b61120ce243537f8",
            "9afe2bc392374f5cbae77caeceadd5ea",
            "ae41b1a6c31845c78ebfe614c81280af",
            "5cc380403abf4befb6d6d2bdefe36584",
            "40a873165e254f6eb68d8430c9fa7c1f",
            "880877cb8bf14f49a3d6218d488f3b51",
            "b5b9c0d198f34d7e8a91e19a12e72b38",
            "d149f4711c5e4aadaa9fc743c378bca7",
            "38daa98113ce4803959f5c062f8e6307",
            "2f195ec9962e4db8adb7d833d26d2f12",
            "9b3873b15de44e92a07a7701c9d3af8f"
          ]
        },
        "outputId": "1a016b26-0299-4504-aef6-c7044948f0cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "554620cab13c4c59b61120ce243537f8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 3\n",
        "from google.colab import files\n",
        "\n",
        "# Upload train and validation JSON files\n",
        "print(\"Please upload qg_train_v0.json\")\n",
        "uploaded = files.upload()\n",
        "print(\"Please upload qg_valid_v0.json\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load JSON data\n",
        "with open(\"qg_train_v0.json\", \"r\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open(\"qg_valid_v0.json\", \"r\") as f:\n",
        "    valid_data = json.load(f)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(\"Train examples:\", len(train_data))\n",
        "print(\"Valid examples:\", len(valid_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "UQdZrEibwnvq",
        "outputId": "4c826bb6-70ae-455a-d74f-b353ad4bfe88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload qg_train_v0.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-32beabdd-6364-4159-9365-83f709cfd17c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-32beabdd-6364-4159-9365-83f709cfd17c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving qg_train_v0.json to qg_train_v0 (4).json\n",
            "Please upload qg_valid_v0.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dc45d005-a8a4-4af2-a39e-53023ca9a85e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dc45d005-a8a4-4af2-a39e-53023ca9a85e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving qg_valid_v0.json to qg_valid_v0 (4).json\n",
            "Train examples: 192\n",
            "Valid examples: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate dataset and count questions\n",
        "def validate_data(data, name):\n",
        "    total_questions = 0\n",
        "    invalid_entries = []\n",
        "    normal_questions = 0\n",
        "    cloze_questions = 0\n",
        "    for i, item in enumerate(data):\n",
        "        questions = item.get('questions', [])\n",
        "        total_questions += len(questions)\n",
        "        for q in questions:\n",
        "            if not q.get('hl_context') or not q.get('question', {}).get('normal_format'):\n",
        "                invalid_entries.append((i, q))\n",
        "            if q.get('question', {}).get('normal_format'):\n",
        "                normal_questions += 1\n",
        "            if q.get('question', {}).get('cloze_format'):\n",
        "                cloze_questions += 1\n",
        "    if invalid_entries:\n",
        "        logger.warning(f\"Invalid entries in {name}: {invalid_entries}\")\n",
        "    else:\n",
        "        logger.info(f\"{name} dataset validated successfully.\")\n",
        "    logger.info(f\"Total questions in {name}: {total_questions} (Normal: {normal_questions}, Cloze: {cloze_questions})\")\n",
        "    return total_questions, normal_questions, cloze_questions\n",
        "\n",
        "train_total, train_normal, train_cloze = validate_data(train_data, \"train\")\n",
        "valid_total, valid_normal, valid_cloze = validate_data(valid_data, \"valid\")\n",
        "\n",
        "# Print sizes\n",
        "print(\"Train items:\", len(train_data))\n",
        "print(\"Train questions:\", train_total, f\"(Normal: {train_normal}, Cloze: {train_cloze})\")\n",
        "print(\"Valid items:\", len(valid_data))\n",
        "print(\"Valid questions:\", valid_total, f\"(Normal: {valid_normal}, Cloze: {valid_cloze})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTC0LTt-7RNE",
        "outputId": "48dce9aa-e5ae-430c-8b67-031401c94711"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train items: 192\n",
            "Train questions: 2726 (Normal: 2726, Cloze: 2726)\n",
            "Valid items: 49\n",
            "Valid questions: 671 (Normal: 671, Cloze: 671)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 4\n",
        "def tokenized_data(source_text, target_text, data_args, tokenizer):\n",
        "    padding = \"max_length\" if data_args.pad_to_max_length else False\n",
        "    model_inputs = tokenizer(source_text, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(target_text, max_length=data_args.max_target_length, padding=padding, truncation=True)\n",
        "    if data_args.ignore_pad_token_for_loss:\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "mfQ7-PwMpwSn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 5\n",
        "class CustomDS(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data['input_ids'])\n",
        "    def __getitem__(self, idx):\n",
        "        return {k: v[idx] for k, v in self.data.items()}"
      ],
      "metadata": {
        "id": "mVh4pxSC1G5o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 6\n",
        "def process_qg_agno_openstax(data, data_args, tokenizer):\n",
        "    all_questions = list(chain(*[d['questions'] for d in data]))\n",
        "    source_text = []\n",
        "    target_text = []\n",
        "    for q in all_questions:\n",
        "        if not q.get('question', {}).get('normal_format'):\n",
        "            continue\n",
        "        hl_context = q.get('hl_context', '').replace('<hl>', '').strip()\n",
        "        if not hl_context:\n",
        "            hl_context = \"No context provided\"\n",
        "        inp_txt = f\"context {hl_context}: \"\n",
        "        out_txt = q['question']['normal_format']\n",
        "        source_text.append(inp_txt)\n",
        "        target_text.append(out_txt)\n",
        "\n",
        "    logger.info(f\"Processed {len(source_text)} normal question-context pairs\")\n",
        "    if data_args.is_debug_mode > 0:\n",
        "        source_text = source_text[:50]\n",
        "        target_text = target_text[:50]\n",
        "\n",
        "    model_inputs = tokenized_data(source_text, target_text, data_args, tokenizer)\n",
        "    return CustomDS(model_inputs)\n"
      ],
      "metadata": {
        "id": "B0-t6c291NNB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 7\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    task: str = \"qg_agno\"\n",
        "    max_source_length: int = 512\n",
        "    max_target_length: int = 48\n",
        "    pad_to_max_length: bool = False\n",
        "    ignore_pad_token_for_loss: bool = True\n",
        "    is_debug_mode: int = -1"
      ],
      "metadata": {
        "id": "hLeQdvjI-kAt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 8\n",
        "set_seed(42)\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "checkpoint_dir = \"./results\"\n",
        "checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")] if os.path.exists(checkpoint_dir) else []\n",
        "print(\"Available checkpoints:\", checkpoints)\n",
        "\n",
        "if checkpoints:\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, max(checkpoints, key=lambda x: int(x.split('-')[1])))\n",
        "    try:\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
        "        print(f\"Checkpoint {checkpoint_path} loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint {checkpoint_path}: {e}\")\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "        print(f\"Using base model {model_name} due to checkpoint error.\")\n",
        "else:\n",
        "    print(f\"No checkpoints found in {checkpoint_dir}, using base model {model_name}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kz8xHY5-xwa",
        "outputId": "8b0f34ed-974c-438e-bcf3-9b4b4af6d0bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available checkpoints: []\n",
            "No checkpoints found in ./results, using base model t5-small.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 9\n",
        "data_args = DataTrainingArguments()\n",
        "train_dataset = process_qg_agno_openstax(train_data, data_args, tokenizer)\n",
        "valid_dataset = process_qg_agno_openstax(valid_data, data_args, tokenizer)"
      ],
      "metadata": {
        "id": "bq8uAoIv-yrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2684869-7faf-45d3-858f-f3f6726b4052"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 10\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=6.25e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.1,\n",
        "    adam_epsilon=1e-8,\n",
        "    max_grad_norm=1.0,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=10,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    warmup_steps=250,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    predict_with_generate=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=[],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_rougeLsum\",  # instead of eval_loss\n",
        "    greater_is_better=True,\n",
        ")"
      ],
      "metadata": {
        "id": "cuQcxnxS-21K"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 11\n",
        "\n",
        "from datasets import load_metric\n",
        "import sacrebleu\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "metric_rouge = load_metric(\"rouge\")\n",
        "metric_meteor = load_metric(\"meteor\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    # 🔒 Fix OverflowError\n",
        "    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n",
        "\n",
        "    # Decode\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    def normalize_text(text):\n",
        "        text = text.lower().strip()\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s?]', '', text)\n",
        "        return text\n",
        "\n",
        "    # ROUGE\n",
        "    rouge_result = metric_rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    result = {k: round(v.mid.fmeasure * 100, 4) for k, v in rouge_result.items()}\n",
        "\n",
        "    # BLEU\n",
        "    bleu_scores = [sacrebleu.corpus_bleu([pred], [[ref]]).score for pred, ref in zip(decoded_preds, decoded_labels)]\n",
        "    result[\"bleu\"] = round(np.mean(bleu_scores), 4)\n",
        "\n",
        "    # METEOR\n",
        "    meteor_result = metric_meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result[\"meteor\"] = round(meteor_result[\"meteor\"] * 100, 4)\n",
        "\n",
        "    # EM\n",
        "    em_scores = [1 if normalize_text(pred) == normalize_text(ref) else 0 for pred, ref in zip(decoded_preds, decoded_labels)]\n",
        "    result[\"em\"] = round(np.mean(em_scores) * 100, 4)\n",
        "\n",
        "    # F1\n",
        "    def compute_f1(pred, ref):\n",
        "        pred_tokens = set(word_tokenize(normalize_text(pred)))\n",
        "        ref_tokens = set(word_tokenize(normalize_text(ref)))\n",
        "        common = pred_tokens & ref_tokens\n",
        "        if not pred_tokens or not ref_tokens:\n",
        "            return 0.0\n",
        "        precision = len(common) / len(pred_tokens)\n",
        "        recall = len(common) / len(ref_tokens)\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    f1_scores = [compute_f1(pred, ref) for pred, ref in zip(decoded_preds, decoded_labels)]\n",
        "    result[\"f1\"] = round(np.mean(f1_scores) * 100, 4)\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzLenbZv--17",
        "outputId": "f915570f-fdde-4839-ef27-17973cad21da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 12\n",
        "import shutil\n",
        "shutil.rmtree(\"./results\", ignore_errors=True)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(\n",
        "        early_stopping_patience=2,\n",
        "        early_stopping_threshold=0.001\n",
        "    )],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"./final_model\")\n",
        "print(\"Final model saved to ./final_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "Tm218O_X_By_",
        "outputId": "9c228d3c-4544-439d-f296-aaeee1e4c9f1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='344' max='850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [344/850 07:59 < 11:49, 0.71 it/s, Epoch 4/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Meteor</th>\n",
              "      <th>Em</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.136800</td>\n",
              "      <td>2.224175</td>\n",
              "      <td>33.174200</td>\n",
              "      <td>12.923000</td>\n",
              "      <td>28.526400</td>\n",
              "      <td>28.506000</td>\n",
              "      <td>9.971800</td>\n",
              "      <td>26.515700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>38.247400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.141800</td>\n",
              "      <td>2.221863</td>\n",
              "      <td>33.375700</td>\n",
              "      <td>13.198200</td>\n",
              "      <td>28.737600</td>\n",
              "      <td>28.704700</td>\n",
              "      <td>10.030700</td>\n",
              "      <td>26.831000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>38.489800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.059900</td>\n",
              "      <td>2.226318</td>\n",
              "      <td>32.728900</td>\n",
              "      <td>12.530100</td>\n",
              "      <td>28.120800</td>\n",
              "      <td>28.055400</td>\n",
              "      <td>9.650200</td>\n",
              "      <td>26.079900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.850500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.000800</td>\n",
              "      <td>2.221431</td>\n",
              "      <td>33.222200</td>\n",
              "      <td>13.106300</td>\n",
              "      <td>28.680000</td>\n",
              "      <td>28.641300</td>\n",
              "      <td>10.036200</td>\n",
              "      <td>26.663200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>38.213500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final model saved to ./final_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 13\n",
        "\n",
        "import random\n",
        "# Identify checkpoints\n",
        "checkpoint_dir = \"./results\"\n",
        "checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")] if os.path.exists(checkpoint_dir) else []\n",
        "print(\"Available checkpoints:\", checkpoints)\n",
        "\n",
        "# Load the best available model\n",
        "try:\n",
        "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1])) if checkpoints else None\n",
        "    model_path = os.path.join(checkpoint_dir, latest_checkpoint) if latest_checkpoint else \"./final_model\"\n",
        "\n",
        "    if os.path.exists(os.path.join(model_path, \"pytorch_model.bin\")):\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "        print(f\"Loaded {model_path} for evaluation.\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Checkpoint incomplete. Falling back.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading {model_path}: {e}\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"./final_model\")\n",
        "    print(\"Loaded final_model as fallback.\")\n",
        "\n",
        "# Build trainer again with model\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", results)\n",
        "\n",
        "# Run prediction\n",
        "raw_preds = trainer.predict(valid_dataset)\n",
        "raw_pred_ids = np.clip(raw_preds.predictions, 0, tokenizer.vocab_size - 1)\n",
        "decoded_preds = tokenizer.batch_decode(raw_pred_ids, skip_special_tokens=True)\n",
        "\n",
        "# For context retrieval\n",
        "def get_contexts_from_dataset(dataset):\n",
        "    return [tokenizer.decode(inp, skip_special_tokens=True) for inp in dataset.data['input_ids']]\n",
        "\n",
        "# Random sample of 5 predictions\n",
        "indices = random.sample(range(len(decoded_preds)), 5)\n",
        "contexts = get_contexts_from_dataset(valid_dataset)\n",
        "\n",
        "print(\"\\n📋 Sample Predictions with Contexts:\")\n",
        "for i, idx in enumerate(indices, 1):\n",
        "    print(f\"\\n🔹 Sample {i}\")\n",
        "    print(f\"📄 Context:\\n{contexts[idx]}\")\n",
        "    print(f\"❓ Predicted Question:\\n{decoded_preds[idx]}\")\n"
      ],
      "metadata": {
        "id": "1a743lky_ErX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "be29d006-392f-4a45-c464-bdbe4245e8b1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available checkpoints: ['checkpoint-172', 'checkpoint-344']\n",
            "Error loading ./results/checkpoint-344: Checkpoint incomplete. Falling back.\n",
            "Loaded final_model as fallback.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'eval_loss': 2.22186279296875, 'eval_model_preparation_time': 0.0048, 'eval_rouge1': 33.3757, 'eval_rouge2': 13.1982, 'eval_rougeL': 28.7376, 'eval_rougeLsum': 28.7047, 'eval_bleu': 10.0307, 'eval_meteor': 26.831, 'eval_em': 0.0, 'eval_f1': 38.4898, 'eval_runtime': 33.5264, 'eval_samples_per_second': 20.014, 'eval_steps_per_second': 2.505}\n",
            "\n",
            "📋 Sample Predictions with Contexts:\n",
            "\n",
            "🔹 Sample 1\n",
            "📄 Context:\n",
            "context Myeloid stem cells give rise to all the other formed elements, including the erythrocytes ; megakaryocytes that produce platelets ; and a myeloblast lineage that gives rise to monocytes and three forms of granular leukocytes : neutrophils, eosinophils, and basophils.:\n",
            "❓ Predicted Question:\n",
            "Which of the following is not a myeloid stem cell?\n",
            "\n",
            "🔹 Sample 2\n",
            "📄 Context:\n",
            "context Many dictatorships do not align themselves strictly with any particular belief system or ideology ; the goal of this type of regime is usually limited to preserving the authority of the dictator at its helm. The totalitarian dictatorship describes a more ambitious and oppressive style of dictatorship that attempts to control all aspects of its subjects ’ lives. Communist regimes, for instance, are often totalitarian in nature. They may attempt to regulate how many children citizens bear, what religious beliefs they hold, and so forth. They may also demand that citizens publicly demonstrate their faith in the regime by participating in public marches and demonstrations.:\n",
            "❓ Predicted Question:\n",
            "Which of the following is not a totalitarian regime?\n",
            "\n",
            "🔹 Sample 3\n",
            "📄 Context:\n",
            "context As discussed in the section on mental health, many mental health disorders can be debilitating, affecting a person ’ s ability to cope with everyday life. This can affect social status, housing, and especially employment. According to the Bureau of Labor Statistics ( 2011 ), people with a disability had a higher rate of unemployment thaN people without a disability in 2010 : 14.8 percent to 9.4 percent. This unemployment rate refers only to people actively looking for a job. In fact, eight out of 10 people with a disability are considered “ out of the labor force ; ” that is, they do not have jobs and are not looking for them. The combination of this population and the high unemployment rate leads to an employment-population ratio of 18.6 percent among those with disabilities. The employment-population ratio for people without disabilities was much higher, at 63.5 percent ( U. S. Bureau of Labor Statistics 2011 ). People with disabilities can be stigmatized by their illness. Stigmatization means that their identity is spoiled ; they are labeled as different, discriminated against, and sometimes even shunned. They are labeled ( as an interactionist might point out ) and ascribed a master status ( as a functionalist might note ), becoming “ the blind girl ” or “ the boy in the wheelchair ” instead of someone afforded a full identity by society. This can be especially true for people who are disabled due to mental illness or disorders.:\n",
            "❓ Predicted Question:\n",
            "What is the rate of unemployment among people with a disability?\n",
            "\n",
            "🔹 Sample 4\n",
            "📄 Context:\n",
            "context The concentration of hydrogen ions dissociating from pure water is 1  10 - 7 moles H + ions per liter of water. Moles ( mol ) are a way to express the amount of a substance ( which can be atoms, molecules, ions, etc ), with one mole being equal to 6.02 x 10 23 particles of the substance. Therefore, 1 mole of water is equal to 6.02 x 10 23 water molecules. The pH is calculated as the negative of the base 10 logarithm of this concentration. The log 10 of 1  10 - 7 is -7.0, and the negative of this number ( indicated by the “ p ” of “ pH ” ) yields a pH of 7.0, which is also known as neutral pH. The pH inside of human cells and blood are examples of two areas of the body where near-neutral pH is maintained. Non-neutral pH readings result from dissolving acids or bases in water. Using the negative logarithm to generate positive integers, high concentrations of hydrogen ions yield a low pH number, whereas low levels of hydrogen ions result in a high pH. An acid is a substance that increases the concentration of hydrogen ions ( H + ) in a solution, usually by having one of its hydrogen atoms dissociate. A base provides either hydroxide ions ( OH – ) or other negatively charged ions that combine with hydrogen ions, reducing their concentration in the solution and thereby raising the pH. In cases where the base releases hydroxide ions, these ions bind to free hydrogen ions, generating new water molecules.:\n",
            "❓ Predicted Question:\n",
            "Which of the following is correct about a base?\n",
            "\n",
            "🔹 Sample 5\n",
            "📄 Context:\n",
            "context The Republican majority in Congress by now despised the president, and they wanted to prevent him from interfering in congressional Reconstruction. To that end, Radical Republicans passed two laws of dubious constitutionality. The Command of the Army Act prohibited the president from issuing military orders except through the commanding general of the army, who could not be relieved or reassigned without the consent of the Senate. The Tenure of Office Act, which Congress passed in 1867, required the president to gain the approval of the Senate whenever he appointed or removed officials. Congress had passed this act to ensure that Republicans who favored Radical Reconstruction would not be barred or stripped of their jobs. In August 1867, President Johnson removed Secretary of War Edwin M. Stanton, who had aligned himself with the Radical Republicans, without gaining Senate approval. He replaced Stanton with Ulysses S. Grant, but Grant resigned and sided with the Republicans against the president. Many Radical Republicans welcomed this blunder by the president as it allowed them to take action to remove Johnson from office, arguing that Johnson had openly violated the Tenure of Office Act. The House of Representatives quickly drafted a resolution to impeach him, a first in American history.:\n",
            "❓ Predicted Question:\n",
            "Which of the following was not a violation of the Tenure of Office Act?\n"
          ]
        }
      ]
    }
  ]
}